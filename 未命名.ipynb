{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data_processor import series_to_supervised,evaluate\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据，检查数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "names = locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list(os.listdir(dir_NWP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_power = 'power_detrended'\n",
    "\n",
    "name_index_power = pd.Series(os.listdir(dir_power)).map(lambda x:x[:-4])\n",
    "\n",
    "for i in name_index_power:\n",
    "    filename = 'power_detrended/'+str(i)+'.csv'\n",
    "    names['data_%s' % i] = pd.read_csv(filename, index_col=3, parse_dates=True)\n",
    "    names['power_%s' % i] = pd.DataFrame(names['data_%s' % i]['power_with_trend'])\n",
    "    names['power_supervised_%s' % i] = series_to_supervised(names['power_%s' % i], 48, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_NWP = 'nwp_refill'\n",
    "\n",
    "name_index_NWP = list(os.listdir(dir_NWP))\n",
    "name_index_NWP.remove('.DS_Store')\n",
    "name_index_NWP = pd.Series(name_index_NWP).map(lambda x:x[3:-4])\n",
    "\n",
    "for i in name_index_NWP:\n",
    "    filename = 'nwp_refill/CN0'+str(i)+'.csv'\n",
    "    names['NWP_%s' % i] = pd.read_csv(filename, index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power_66.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34992"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(power_66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 功率数据集从2017.1.2至2018.12.31，取其中7.00-19.00的部分，长度34992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NWP_016.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(NWP_016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NWP数据集从2017.1.2至2019.3.7，比功率要多，取与功率数据相同的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NWP_66 = NWP_016.loc[power_66.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NWP_66.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据NWP的云量，初步确定每个时刻的天气状况的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = pd.DataFrame(NWP_016.cloud_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud['cloud_diff'] = cloud.diff(1)\n",
    "cloud = cloud.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断标准：云量大于90或者小于5 且 变化率小于1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud['fluctuate'] = [0 if ((cloud.cloud_amount[i]>95 or cloud.cloud_amount[i]<5) and abs(cloud.cloud_diff[i])<1) \n",
    "                      else 1 \n",
    "                      for i in range(len(cloud))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = cloud.loc[power_66.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_stable = [x for x in cloud[cloud.fluctuate == 0].index if x < datetime.datetime(2019,1,1)]\n",
    "index_fluc = [x for x in cloud[cloud.fluctuate == 1].index if x < datetime.datetime(2019,1,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察此判断标准下是否大致合理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 波动小的样本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19708"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15284"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_fluc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 观察具体曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_stable = power_66.loc[index_stable]\n",
    "power_fluc = power_66.loc[index_fluc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayuan/anaconda3/lib/python3.6/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a25d26898>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 15\n",
    "plt.plot(power_66[48*(k-1):48*k]/20, label='power')\n",
    "plt.plot(cloud.cloud_amount.loc[power_66[48*(k-1):48*k].index]/100, label='cloud')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "power_observe = power_66.copy()\n",
    "power_observe['fluc'] = cloud.loc[power_66.index]['fluctuate']\n",
    "\n",
    "power_observe['stable'] = [-(power_observe['fluc'][i]-1)*power_observe['power_with_trend'][i] for i in range(len(power_observe))]\n",
    "power_observe['fluc'] = [(power_observe['fluc'][i])*power_observe['power_with_trend'][i] for i in range(len(power_observe))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a34f9e358>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAEGCAYAAAAt7EI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3RU1dfG8e/JpAFpdIEEEjoEQoDQJaGEjgWlgzQVAVERUbEr6CsqUoWIoiCIgjQpUn50kCJFQg3dCAHB0FIgPef9Y5JISSBAhjsz2Z+1ZsHM3LnzJEDYc865+yitNUIIIYQQ4uFyMDqAEEIIIUR+JEWYEEIIIYQBpAgTQgghhDCAFGFCCCGEEAaQIkwIIYQQwgCORge4V8WKFdO+vr5GxxBCCCGEuKs9e/Zc1FoXz+45myvCfH192b17t9ExhBBCCCHuSin1d07PyXSkEEIIIYQBpAgTQgghhDCAFGFCCCGEEAawuTVhQgghhL1ISUkhKiqKxMREo6OIB+Tq6oq3tzdOTk65fo0UYUIIIYRBoqKicHd3x9fXF6WU0XHEfdJac+nSJaKiovDz88v162Q6UgghhDBIYmIiRYsWlQLMximlKFq06D2PaEoRJoQQQhhICjD7cD9/jjIdKYTIPy7/BRHLICkud8ebnKF0bfCpD64els0mhMh3pAgTQti3hCtwaDHsmwdndmQ8mNtPrDrjcAcoVQvKNYFyjaFsIyhYxBJphRD5iBRhQgj7k5oMJ9bAvrlwbBWkJUPxqtDyAwjoCp7euTtP8jWI2gV/bzPfdk2H7V+ZnytR3VyQlWtsLs7cH7Hc1yNEPhYZGcm2bdvo2bPnHY/buHEjY8eOZfny5bc9l7nbTrFixSwV875IESaEsA9aQ9Ru2D8XDi6ChMtQqDjUew4CuplHsu51zYZzISjfzHwDSE2Cs3/C31vNRdm+uebCDKBYZajZxVzkFfbNsy9LCHuUmpqKo2PuSpDIyEh++umnuxZhtkiKMCGE7Uq+Bmd2QuTv5inHyyfB0RWqdoCA7lChBZjy8MecowuUa2S+AaSlwvn95qLs2GrY8In5VraRufDzfxIKFM679xd27aNlhzh8LjZPz1m9tAcfPOZ/x2MiIyNp27YtDRo0YO/evVSuXJlZs2axfft2RowYQWpqKvXq1SMsLIx9+/YxZswYFi1axJIlS+jevTsxMTGkp6dTvXp1Tp06xcmTJ3nxxReJjo6mYMGCfPvtt1StWpV+/fpRpEgR9u7dS506dfjyyy9vy7Jp0yZeeeUVwLzQffPmzYwcOZKIiAgCAwPp27cvnTp14plnnuHatWsAfPXVVzRu3BiA2NhYOnXqxNGjRwkODmbq1Kk4ONx8DeKPP/7IpEmTSE5OpkGDBkydOhWTyZQX3+57JkWYEMJ2JFyF0zv+G4n6JxzSU0GZzNOCTYdDtccf3iJ6kyOUqWO+NX4Jrp6BA7+YR8iWD4OVb0CVduaCsGIoODo/nFxC3KOjR4/y3Xff0aRJEwYMGMC4ceOYNm0a69ato3LlyvTp04ewsDCGDh3K3r17AdiyZQs1atRg165dpKam0qBBAwAGDhzI119/TaVKlfjjjz8YMmQI69evB+DYsWOsXbs2x6Jn7NixTJkyhSZNmhAfH4+rqytjxoy5aZrx+vXrrFmzBldXV44fP06PHj3YvXs3ADt37uTw4cOUK1eOtm3bsmjRIjp37px1/oiICObNm8fWrVtxcnJiyJAhzJkzhz59+ljse3snUoQJIaxXfDSczliPFbkVLhwEtPmqxTJ1ockr5uLLpwG4uBudFrx8oOlr8OhwOLcX9s+DAwvg8BIoUARqPA21upuzS1sCcYu7jVhZko+PD02aNAGgd+/ejB49Gj8/PypXrgxA3759mTJlCsOGDaNixYpERESwc+dOhg8fzubNm0lLS6Np06bEx8ezbds2unTpknXupKSkrN936dLljqNOTZo0Yfjw4fTq1YunnnoKb+/b12+mpKQwdOhQwsPDMZlMHDt2LOu5+vXrU758eQB69OjB77//flMRtm7dOvbs2UO9evUASEhIoESJEvfzLcsTUoQJIYyTFA8xZ8wjSDGn4erpjN9nPBZ/3nycYwFzm4jmb5uLrjJ1wamAsdnvRKn/Rshafwwn15tHx/bOhl3fgqfPf1dalmsCRStIUSYMdS89rpo2bcrKlStxcnIiNDSUfv36kZaWxtixY0lPT8fLy4vw8PBsX1uoUKE7nnvkyJF06NCBFStW0LBhQ9auXXvbMePHj6dkyZLs27eP9PR0XF1dc/w6br2vtaZv3758+umnuf1yLUqKMCHEw5GWal63dfhXc7EVc8bcPuJGDk7mKxe9fMzTd8UqmYuUUrVsdyrP5ASV25hviTFweKn5ys2T68wXEQAUKvFfQebbBIpXAwfppS0entOnT7N9+3YaNWrEzz//TGhoKNOmTePEiRNUrFiR2bNnExISAkBwcDB9+vShT58+FC9enEuXLnH+/Hn8/f1RSuHn58f8+fPp0qULWmv2799PrVq1cpXj5MmT1KxZk5o1a7J9+3aOHDmCj48PcXH/9faLiYnB29sbBwcHfvjhB9LS0rKe27lzJ3/99RflypVj3rx5DBw48Kbzt2zZkieeeIJXX32VEiVKcPnyZeLi4ihXrlwefBfvnRRhQgjLSksxjwL9Pg4unwKvsuZ2Ed71zMWWp4/5MU8fcCtp38WHqyfUecZ80xounfhvfVvkVnOBCuDq9V/7i4Bu4GbcdInIH6pVq8YPP/zACy+8QKVKlZg4cSINGzakS5cuWQvzBw0aBECDBg24cOECwcHBAAQEBFCiRImsUac5c+YwePBgPv74Y1JSUujevXuui7AJEyawYcMGTCYT1atXp127djg4OODo6EitWrXo168fQ4YM4emnn2b+/Pk0b978ptG1Ro0aMXLkSA4cOEBwcDCdOnW66fzVq1fn448/pnXr1qSnp+Pk5MSUKVMMK8KU1tqQN75fQUFBOnMBnhDCiqUkQviP8PsE86jXIwEQ/DpU7WjfhdaDuHo6oydZRmF26QQEPQsdxxmdTFhIREQE1apVMzRDZGQkHTt25ODBg4bmsAfZ/XkqpfZorYOyO15GwoQQeSv5GuyZCVsnmdd0edeDDuOgUitZ93Q3XmXNt1rdzfcnB5n7nQkh7JJFizClVFtgImACpmutx9zy/HigecbdgkAJrbWXJTMJISwkMTajo/wUuH4RfJvCU9+AX7AUX/fL1cP8fRXCgnx9fR/6KNiMGTOYOHHiTY81adKEKVOmPNQcRrNYEaaUMgFTgFZAFLBLKbVUa3048xit9as3HP8SUNtSeYQQFnL9Muz8BnaEQeJVqNgKgkdA2YZGJ7N9Lh6QJEWYsD/9+/enf//+RscwnCVHwuoDJ7TWpwCUUnOBJ4DDORzfA/jAgnmEEHkpPhp2TIGd0yE5zrzWK3gElJbPUnnGxR3i/snVoalp6Zgc1D21GhBCGMuSRVgZ4MwN96OABtkdqJQqB/gB6y2YRwiRF2LPwbbJsHsGpCZCjafMDUpLGtdo0m7lcjoyNjGFxp+up1JJN15qUZHmVUpIMSaEDbBkEZbdT4CcLsXsDizQWqdl96RSaiAwEKBs2bJ5k04IcW+u/A1bJ8DeHyE9zbx4/NFXzb28hGW4eOZqOvJCTCLxSakcPhfLgJm78S/twUstKtK6+iM4OEgxJoS1smQRFgX43HDfGziXw7HdgRdzOpHW+hvgGzC3qMirgEKIXLh0EraMMzcWVQ4Q2AseHQaFfY1OZv9c3CE53lz0OuS81UtsYioAk3vUJiYhhakbTzLoxz+pVMKNoS0q0jGgNCYpxoSwOpZs1rMLqKSU8lNKOWMutJbeepBSqgpQGNhuwSxCiHv1bwQseBa+CoKDC6De8/ByODw2QQqwhyVzI/KkuDseFpeYAkBRN2e6BPmwdngIE7sHohS8Mjec0HGb+GX3GVLS0i2dWIgHkpSURGhoKIGBgcybN49mzZpxP71Br169ytSpUx8oy8yZMzl3Lqexo7xhsSJMa50KDAVWAxHAL1rrQ0qpUUqpx284tAcwV9ta11gh7FV6GmweC2FN4NgqaPwSDDsA7caAZxmj0+UvLplF2J2nJOMyRsLcXZ0AMDkonggsw6pXgvm6d10KuZh4Y8F+mn2xkdk7/iYxJduVH0I8NKmpqdk+vnfvXlJSUggPD6dbt273fX5bKcIs2idMa70CWHHLY+/fcv9DS2YQQtyD2H9g0fMQuQVqPA3tx0LBIkanyr9c3M2/3nUkLLMIu/lHuoODom2NR2jjX5KNR6OZtP447/16kCnrT/BVz9oE+cqfrVVZORLOH8jbcz5S0/wB6g4iIyNp27YtDRo0YO/evVSuXJlZs2axfft2RowYkbVtUVhYGPv27WPMmDEsWrSIJUuW0L17d2JiYkhPT6d69eqcOnWKkydP8uKLLxIdHU3BggX59ttvqVq1Kv369aNIkSLs3buXOnXq8OWXX96U499//6V3795ER0cTGBjIwoULb3rezc2N+Ph4ABYsWMDy5cuZOXMmFy5cYNCgQZw6dQqAsLAwJk2axMmTJwkMDKRVq1Z06NCBsWPHsnz5cgCGDh1KUFAQ/fr1Y9SoUSxbtoyEhAQaN27MtGnTWLhwIbt376ZXr14UKFCA7du3c/jwYYYPH058fDzFihVj5syZlCpV6oH+eGTvECGE2dFVENYYzu6BJ6bA099JAWa0zOnIu1whmTkdmTkSdiulFM2rlmDR4Mb89FwDCjib6PntH/y692yexhW26+jRowwcOJD9+/fj4eHBuHHj6NevH/PmzePAgQOkpqYSFhZGnTp12Lt3LwBbtmyhRo0a7Nq1iz/++IMGDcwNEAYOHMjkyZPZs2cPY8eOZciQIVnvc+zYMdauXXtbAQZQokQJpk+fTtOmTQkPD6dChQq5yv7yyy8TEhLCvn37+PPPP/H392fMmDFUqFCB8PBwvvjiizu+fujQoezatYuDBw+SkJDA8uXL6dy5M0FBQcyZM4fw8HAcHR156aWXWLBgAXv27GHAgAG88847uf325ki2LRIiv0tNgjXvwx9fmz81d54hVzxaCxdP86+5mI50UFDIOefF+2AuxhpXLMbiIY0Z9OMehs0L59TFa7waWklaWliDu4xYWZKPjw9NmjQBoHfv3owePRo/Pz8qV64MQN++fZkyZQrDhg2jYsWKREREsHPnToYPH87mzZtJS0ujadOmxMfHs23bNrp06ZJ17qSkpKzfd+nSBZPpzn9P79X69euZNWsWACaTCU9PT65cuZLr12/YsIHPP/+c69evc/nyZfz9/XnsscduOubo0aMcPHiQVq1aAZCWlvbAo2AgRZgQ+Vv0MVg4wDwF0mAwtPoIHF2MTiUy5Xo6MgU3F8dcF1JeBZ2ZNaAB7yw+wKR1x/nr4jW+6ByAq1Pe/ucobMe9FOFNmzZl5cqVODk5ERoaSr9+/UhLS2Ps2LGkp6fj5eVFeHh4tq8tVKhQnmRMTEy8p9c6OjqSnv7fhSmZr09MTGTIkCHs3r0bHx8fPvzww2zPrbXG39+f7dvz9hpCmY4UIj/SGv6cDd+EmJuv9phn/hQuBZh1yZqOjLnjYXGJqTlORebE2dGBzzsH8Gbbqizbd46e3+7gYnzS3V8o7NLp06ezCoyff/6Z0NBQIiMjOXHiBACzZ88mJCQEgODgYCZMmECjRo0oXrw4ly5d4siRI/j7++Ph4YGfnx/z588HzMXLvn378iRjyZIliYiIID09ncWLF2c93rJlS8LCwgDzCFVsbCzu7u7Exf334aVcuXIcPnyYpKQkYmJiWLduHfBfMVasWDHi4+NZsGBB1mtuPEeVKlWIjo7O+h6lpKRw6NChB/6apAgTIr9JjIGFz8LSoVCmLgzaClXaGp1KZCeXV0fGJqbetig/N5RSDG5WgbBedTj8TyxPTtnKsQt3HnUT9qlatWr88MMPBAQEcPnyZV599VVmzJhBly5dqFmzJg4ODgwaNAiABg0acOHCBYKDgwEICAggICAga6Rqzpw5fPfdd9SqVQt/f3+WLFmSJxnHjBlDx44dadGixU1TgRMnTmTDhg3UrFmTunXrcujQIYoWLUqTJk2oUaMGr7/+Oj4+PnTt2pWAgAB69epF7drm7dW8vLx4/vnnqVmzJk8++ST16tXLOm+/fv0YNGgQgYGBpKWlsWDBAt58801q1apFYGAg27Zte+CvSdlaZ4igoCB9Pz1DhBBA1G5YMABioqDFO9Bk2B2bgAqDaQ2jipqb47Z8P8fDuk3bjtbwy6BG9/1W+6Ou8uwPu0lMTuOrXnUIqVz8vs8lci8iIoJq1aoZmiEyMpKOHTty8OBBQ3PYg+z+PJVSe7TWQdkdLyNhQuQH6emw5Uv4rjWgYcBq836PUoBZN6VytX9k3H2OhN0owNuLJS82wbtIQQbM3MXsHX8/0PmEEHcnRZgQ9i72H5j9JKwbBdUfhxe2gE+9u79OWAcXj7tfHZmU8sBFGEBprwLMH9SIkMrFee/Xg3y07BBp6bY1WyLuna+v70MfBZsxYwaBgYE33V58McfdC+2WXB0phD07thp+HQwpCfD4ZKj9jHl0RdgOF4+7Xh0Zm3DvC/Nz4ubiyLd9gvjktwi+3/oXf128xviugRQu5Jwn5xcCoH///vTv39/oGIaTkTAh7FFqEqx6C37qCu6lYeBGqNNHCjBbdJfpSK018UmpeBTIu8/UJgfF+49V5+Mna7D1xEXaTdzCjlOX8uz8QggzKcKEsDcXj8P0lrBjKtR/AZ5bC8WrGJ1K3C8XD0jKuUXF9eQ00tJ1no2E3ah3w3IsHtIko8P+DsatOUaqbAIuRJ6RIkwIe6E17P0RpgVDzFnoMRfafw5OrkYnEw/Cxf2O05E57RuZV2qU8WT5S4/SqbY3k9Ydp/s3O4i6ct0i7yVEfiNFmBD2IDEGFj4HS1409/4avBWqtDM6lcgLd5mOvNu+kXmhkIsjX3atxYRugRw5H0f7iVtYeeAfi72fMNaHH37I2LFj8+RczZo1Q9pK5UyKMCFs3bm98HVTOLQYWrwLfZaAR2mjU4m8knl1ZA49HWMtPBJ2oydrl+G3lx/Fr1ghBs/5k7cXHyAhOc3i7yuEvZIiTAhb9vd2mNkRdDr0XwnBr0vvL3vj4g7pqZCa/V55mSNhHg+hCAMoV7QQ8wc1ZlBIBX764zRPTPmdo+ely74tmzVrFgEBAdSqVYtnnnnmpufCw8Np2LAhAQEBdOrUKWtj7BtHuC5evIivry8ACQkJdO/enYCAALp160ZCQsJD/VpsjbSoEMJWndoEP3cHjzLQd6mMftmrrP0jY8GpwG1P/7cmzHLTkbdydnRgZLuqNKlYlFfn7ePxr37n3Y7V6d2g7D1tBC1u9tnOzzhy+UienrNqkaq8Wf/NHJ8/dOgQn3zyCVu3bqVYsWJcvnyZSZMmZT3fp08fJk+eTEhICO+//z4fffQREyZMyPF8YWFhFCxYkP3797N//37q1KmTp1+PvZGRMCFs0Ym15vYTXuWg/wopwOyZi6f51xwatlp6Yf6dNK1UnFXDmtKwfFHe+/UgL88NJzlVrp60JevXr6dz584UK1YMgCJFimQ9FxMTw9WrV7M27u7bty+bN2++4/k2b95M7969gf/2lBQ5k5EwIWzN0ZXwSx9z24lnlkChokYnEpbk4m7+NccizPIL8++kmJsLM/rVI2zTSb5YfZSE5FSm9KqDi6NMi9+rO41YWYrW+r5GLx0dHUlPNxfciYk3T5XLaGjuWXQkTCnVVil1VCl1Qik1ModjuiqlDiulDimlfrJkHiFs3uElMK83lKwBfZZKAZYf3DgdmY24xFQcFBRyNq7ocXBQvNi8IqOfrMHaiH95ftYeElNkwb4taNmyJb/88guXLpmb8V6+fDnrOU9PTwoXLsyWLVsAmD17dtaomK+vL3v27AFgwYIFWa8JDg5mzpw5ABw8eJD9+/c/lK/DVllsJEwpZQKmAK2AKGCXUmqp1vrwDcdUAt4CmmitryilSlgqjxA2b/98WPwCeAdBr/ng6ml0IvEwuGQUYXcYCXNzcbSK0YdnGpbD2aQYuegA/Wfs4rt+QRR0lgkXa+bv788777xDSEgIJpOJ2rVrZy2yB/jhhx8YNGgQ169fp3z58syYMQOAESNG0LVrV2bPnk2LFi2yjh88eDD9+/cnICCAwMBA6tev/7C/JJtiyX8d9YETWutTAEqpucATwOEbjnkemKK1vgKgtf7XgnmEsF1755h7gPk+am7C6uJmdCLxsGRNR2Z/BWJcYt7tG5kXutUri7OjA6/9so++3+/k+371rCqfuF3fvn3p27dvts8FBgayY8eO2x6vWrXqTaNcH3/8MQAFChRg7ty5lglqhyw5HVkGOHPD/aiMx25UGaislNqqlNqhlGqb3YmUUgOVUruVUrujo6MtFFcIK7V7BiwZAuWbQc9fpADLb+4yHRmbmGrIovw76VTbm8k96rD39FV6f7eTmOspRkcSwipZsgjLbmz81m6DjkAloBnQA5iulPK67UVaf6O1DtJaBxUvXjzPgwphtXZ8DcuHQaU25hEw54JGJxIPm/OdF+bHJqbgUcD6Rpo6BJRiaq86HD4XQ8/pO7hyLdnoSEJYHUsWYVGAzw33vYFz2RyzRGudorX+CziKuSgTQmydCKvehKododuPsgdkfmVyBKdCd5yOfFiNWu9Va/9H+KZPEMf/jafHtzu4GJ9kdCSrpHPYDUHYlvv5c7RkEbYLqKSU8lNKOQPdgaW3HPMr0BxAKVUM8/TkKQtmEsI2bJsMa94H/6egy0xwdDY6kTCSq4d5f9BsxCWmWPWaq+ZVSvB933pEXrpGt2nbuRCbfef//MrV1ZVLly5JIWbjtNZcunQJV9d7+7BssY9PWutUpdRQYDVgAr7XWh9SSo0Cdmutl2Y811opdRhIA17XWl+yVKbcuJhwka/2fsWAGgMo61HWyCgiv9o7B/73Lvh3gqenyzZEwrw4/w7NWq1tTditHq1UjB/612fAzF10m7adn55vSGmv27v/50fe3t5ERUUh651tn6urK97e3vf0Gov+y9VarwBW3PLY+zf8XgPDM25WYV/0PpafWs7iE4tp79ee52s+T3mv8kbHEvnFkRWw9CUo3xw6fSMFmDBz8ch2OlJrTXyS9RdhAA3KF2XWsw3o9/1Ouk7bzs/PN8SniKxxdHJyws/Pz+gYwiCybdEtWpZtyaqnV9Gneh/WnV7Hk0ue5LWNr3H08lGjowl7F7kVFvSHUrXMa8BkClJkcvXI9urI68lppKVrq56OvFHdcoWZ83wD4hJTGThbGroKIUVYNooVKMZrQa+x+unVPFfzObae20rnZZ15ef3LHLp0yOh4wh6dPwA/9wBPH+i1QNpQiJvlMB1p5L6R9yvA24sJ3QOJ+CeWUcsP3/0FQtgxKcLuoLBrYV6u8zKrn17NkFpD2H1hN92Xd2fw2sGE/xtudDxhLy7/BT8+bS68nlksWxGJ2+UwHWn0vpH3q3mVEgxuVoGf/jjNkvCzRscRwjBShOWCp4sngwMH87+n/8crdV7h0MVDPLPyGZ5b/Ry7zu+Sq1rE/Yu7ALM7QVoy9F4EXj53f43If1w9s52OjLXBkbBMr7WqTFC5wry96ACnouONjiOEIaQIuwduzm48V/M5Vj29ihFBIzgZc5IBqwfQb1U/tp3dJsWYuDeJMeYRsPgL0HM+lKhqdCJhrVzcIeUapKXe9HDmSJi19gm7E0eTA5N71sbZ0YEXf9or68NEviRF2H0o6FSQvv59WfnUSt6q/xZn48/ywtoX6LWiFxvPbJRiTNxdSqJ5DVh0BHSbDT71jE4krFnmJt7JN09J/rcmzLamIzOV8izAuK6yPkzkX1KEPQBXR1d6VuvJiqdW8EGjD7iceJmX1r9E1+VdWfP3GtJ1utERhTVKS4UFA+DvbdBpGlQMNTqRsHY57B9piwvzb9W8agkGhcj6MJE/SRGWB5xNznSu3JllnZbxcZOPSUxNZPjG4Ty15Cl+O/UbaekyzC4yaA3LX4Gjv0G7z6BmZ6MTCVvgkrl/5K0jYZnTkbY5EpbptdayPkzkT1KE5SEnByeeqPgEvz7xK581/QylFCO3jOSJJU/w64lfSUlPMTqiMNraD2HvjxD8BjR4weg0wlZkTkfe0qYiNjEFk4OioLNtN/V1MjkwqYesDxP5jxRhFmByMNG+fHsWPr6Q8c3GU8CxAO9tfY/HFj/G/GPzSU5LNjqiMMK2ybB1AgQNgOZvG51G2JI7TEe6uTiilDIgVN4q7SXrw0T+I0WYBTkoB0LLhfJLx1/4qsVXFHEtwqjto2i/qD1zIuaQmCob2eYb4T+Z94Os/gS0Hwt28J+meIiyRsJuX5hvy+vBbtW8agleCCkv68NEviFF2EOglCLEJ4Q57ecwLXQaZdzKMGbnGNotascPh37gesp1oyMKSzq6EpYMBb8QeOpb2Q9S3LusIizmpofjElNs9srInIxoXYW6sj5M5BNShD1ESikal2nMD+1+4Ps231PBqwJjd4+l7cK2TD8wnfhk+YFjd/7eDvP7QakA6D4HHF2MTiRsUQ7TkbF2NhIG5vVhk3vUxknWh4l8QIowg9R7pB7TW09ndrvZ+BfzZ+KfE2m9sDVTw6cSc8unXWGjzh+En7qBp3fGfpDuRicStsrRFRwcs52OtMVGrXdjXh9WS9aHCbsnRZjBAksEEhYaxtwOc6lXsh5h+8Jos7ANcyLmSNNXW3YlEn58CpwLZewHWczoRMKWKZWxf+StC/PtbzoyU4uqJWV9mLB7UoRZCf9i/kxsMZEFjy2gdonajNk5hne3viuL921R/L8w60lITYJnFoFXWaMTCXvg6pHt1ZH2Nh15I1kfJuydFGFWpkqRKkxpOYUhgUNYenIp/Vb14/y180bHErmVGGMeAYu/AL3mQ4lqRicS9sLF/abpSK018Un2XYTJ+jBh7yxahCml2iqljiqlTiilRmbzfD+lVLRSKjzj9pwl89gKB+XA4FqDmdR8EpGxkXRb3o0/L/xpdCxxNymJ8HNP+DpBtUUAACAASURBVDcCus4Gn/pGJxL2xMXzpunI68lppKVru52OzCTrw4Q9s1gRppQyAVOAdkB1oIdSqno2h87TWgdm3KZbKo8tal62OT+1/wl3Z3eeXf0svxz9RdaJWau0VFj4LPz9Ozz5NVSS/SBFHrtlOtIe9o3MrRZVS/JCsHl92NJ954yOI0SeseRIWH3ghNb6lNY6GZgLPGHB97NL5b3K81OHn2hUuhGjd4zmo+0fScd9a6M1LB8GR5ZD288goIvRiYQ9cnG/aSTMXvaNzK0RbapQp6wXby3cz18XrxkdR4g8YckirAxw5ob7URmP3epppdR+pdQCpZRPdidSSg1USu1WSu2Ojo62RFar5uHsweQWk3m+5vMsPL6QAasHEH09/30frNa6UbB3NgS/Dg0HGZ1G2Ktbro6MzUcjYZCxPqxnHZwcHRgy509ZHybsgiWLsOz2Zbl1Lm0Z4Ku1DgDWAj9kdyKt9Tda6yCtdVDx4sXzOKZtMDmYeLnOy4wNGcuxK8fovrw7+6P3Gx1LnNwAv4+DOn2g+TtGpxH2LHM6MmNJQuZImL2vCbtRGa8CfNnFvD5stKwPE3bAkkVYFHDjyJY3cNNkvtb6ktY6KePut0BdC+axC2182zC73WycTE70W9WPxccXGx0p/0q+bp6GLFIe2n0u+0EKy3JxB50GKQnAfyNh9tis9U5aVjOvD5vzx2mWyfowYeMsWYTtAioppfyUUs5Ad2DpjQcopUrdcPdxIMKCeexGlSJVmNthLnVL1uX9be8z4+AMoyPlT5s/NzdlfWwiOBUwOo2wd1n7R5qnJPPjSFimrPVhiw7I+jBh0yxWhGmtU4GhwGrMxdUvWutDSqlRSqnHMw57WSl1SCm1D3gZ6GepPPbGy9WLsNAw2vm2Y9yecSw/tdzoSPnL+QOwdRIE9ga/YKPTiPzA1dP8a2JmEZa/1oTdKHN9mKNJ8aKsDxM2zKJ9wrTWK7TWlbXWFbTWn2Q89r7WemnG79/SWvtrrWtprZtrrY9YMo+9cXRw5ONHP6b+I/V5b+t7bD+33ehI+UN6Gix9GQoUhtajjU4j8ovMvUczGrbGJaZgclAUdDYZGMo4mevDDv8Ty8e/yfowYZukY76NczY5M775ePw8/Xh146scuSx1rMXt/AbO/QntPoOCRYxOI/KLrOnIGMA8Eubm4ojKx2sRM9eH/bhD1ocJ2yRFmB3wcPYgrGUY7s7uDF47mLPxstmtxVw9A+tGQ8VQqPG00WlEfuKaUYTdMB2ZH6cibyXrw4QtkyLMTpQsVJKwlmEkpSUxaM0griZeNTqS/dEafnsN0NBhnFwNKR6ubKYj8+Oi/Fvduj4sKVXWhwnbIUWYHalYuCKTW0zmXPw5Xlr/EompiUZHsi+HFsPx1eZ+YIXLGZ1G5De3XB0ZKyNhWW5cHzZuzTGj4wiRa1KE2Zm6JevyadNP2Re9jzc3v0launwqzBMJV2Dlm1AqEBpIV3xhgMyRsBumI/Nbj7A7aVmtJD3ql+WbzafYFXnZ6DhC5IoUYXaotW9r3qz/JuvPrOfTnZ/Kpt95Yc0HcP0SPD4JTPIfnzCAgwmc3WQ68g7e7VANn8IFGf5LOPFJqUbHEeKupAizU72q9aJ/jf7MOzqP7w5+Z3Qc2xb5O/z5AzQaAqVqGZ1G5GcuHjddHSkjYTcr5OLIl11rEXUlgU9+k97fwvpJEWbHhtUZRofyHZj450SWnFhidBzblJIIy4aBV1lo9pbRaUR+5+IOibForYlPSpWRsGzU8y3CwODy/LzzNBuO/Gt0HCHuSIowO+agHBjdeDQNSjXgw20fsvXsVqMj2Z7fx8Gl49BxPDgXMjqNyO9cPSApjuvJaaSla1mYn4PhrSpTpaQ7byzcz5VryUbHESJHUoTZOSeTExOaTaCCVwVe3fgqhy9JZ+lc+zcCtoyDml3NfcGEMJqLByTF3rBlkYyEZcfF0cS4brW4ej2Z95YcNDqOEDmSIiwfcHN2Y2roVLxcvBiydghRcVFGR7J+6emw7BVwcYO2nxqdRgizjOnI2KzNu2UkLCf+pT0ZFlqZ5fv/Yal00xdWSoqwfKJEwRJ8Hfo1KekpDF47mCuJV4yOZN32zIAzf0Cb/4NCxYxOI4RZxnRknBRhufJCcHlql/XivV8Pcj5G+iYK6yNFWD5S3qs8X7X8KquZa0JqgtGRrFPsOVj7IfgFQ60eRqcR4j8Z05GxMh2ZK44mB8Z1DSQ5NZ03Fu6Xdj3C6ty1CFNKmZRSXzyMMMLyapeozWfBn7E/er80c83JyjcgLRk6TpCtiYR1cfGAlOvEXzd/gJIWFXfnV6wQb7evyuZj0cz547TRcYS4yV2LMK11GlBXKfnfyF6ElgtlZP2RbDizQZq53ipiOUQsg5A3oWgFo9MIcbOMTbwT4829wmQkLHd6NyxH00rF+OS3CCJlk29hRXI7HbkXWKKUekYp9VTmzZLBhGX1rNYzq5nr9APTjY5jHRJjYcXrULIGNH7J6DRC3C5j/8jka1cBWROWW0opPu8cgJNJ8dr8faSlywdPYR1yW4QVAS4BLYDHMm4dLRVKPByZzVwn7Z3E0pNLjY5jvHWjIO4feGwSmGSEQVihjP0jU69fxeSgKOhsMjiQ7SjlWYDRT9Zgz99X+GbzKaPjCAFArj5Gaa3738/JlVJtgYmACZiutR6Tw3GdgflAPa317vt5L3HvMpu5Xky4yAdbP6CYazEal2lsdCxjnNkJu6ZDgxfAu67RaYTIXsZ0ZGpCLG4uJZBVIvfm8Vql+d+hC4xbc5RmVYpTrZSH0ZFEPperkTClVGWl1Dql1MGM+wFKqXfv8hoTMAVoB1QHeiilqmdznDvwMvDHvYYXD06auQKpybD0ZfAoAy3u+NdaCGNlTEfqhBg8CshU5L1SSjH6yRp4FnBm+C/7SE1LNzqSyOdyOx35LfAWkAKgtd4PdL/La+oDJ7TWp7TWycBc4IlsjhsNfA5IExeDZDZz9XTx5MV1L+a/Zq7bJkJ0BHQYmzXdI4RVyijCSIrF3UWmzO9HkULOjHrCn4h/YlmwJ5/9rBNWJ7dFWEGt9c5bHku9y2vKAGduuB+V8VgWpVRtwEdrvfxOJ1JKDVRK7VZK7Y6Ojs5lZHEvMpu5JqclM3jtYK4mXjU60sNx8QRs+gKqPwlV2hmdRog7c80swuJkUf4DaFfjEeqU9WLcmmNcT77bf2VCWE5ui7CLSqkKgIasNVz/3OU12S1WyLokRSnlAIwHXrvbm2utv9FaB2mtg4oXL57LyOJelfcqz+QWkzkXf46h64fafzNXrWH5MHB0hXafGZ1GiLvLGAkzJcdJe4oHoJTinQ7V+Dcuielb/jI6jsjHcluEvQhMA6oqpc4Cw4BBd3lNFOBzw31v4MYNvNyBGsBGpVQk0BBYqpQKymUmYQF1StZhTPCY/NHMde+PELkFWo8C90eMTiPE3Tm6gIMTjqnx0qj1AdUtV4S2/o8wbdNJouOSjI4j8qlcFWEZ67pCgeJAVa31o1rrv+/ysl1AJaWUn1LKGfMasqw+CFrrGK11Ma21r9baF9gBPC5XRxqvVblW9t/MNf5f+N+7ULYx1O5jdBohckcpcPXAOTVepiPzwJvtqpKUms6EtceMjiLyqdxeHXlSKTUHeIabR7dypLVOBYYCq4EI4Bet9SGl1Cil1OP3G1g8HD2r9WRAjQH228x11UhIuQ6PTQQH2UJV2A7t4oFrWrxMR+YBv2KF6NWgLHN3neHEv/FGxxH5UG7/96mOeTqyKDBWKXVKKbX4bi/SWq/QWlfWWlfQWn+S8dj7WuvbOoNqrZvJKJh1eaXOK3Qs35FJeyex5MQSo+PknWP/g4MLoekIKF7Z6DRC3JN0Z3cKkSAjYXnk5ZaVKOBk4rNVR4yOIvKh3BZhaZjbU6QB6cAF4F9LhRLWwUE5MKrxKBqWasiH2z5k69mtRkd6cEnx8NtrUKwKPDrM6DRC3LNUJzfcVYKMhOWRom4uDG5WgTWHL7Dzr8tGxxH5TG6LsFhgAvAX0Fdr3Uhr/YLlYglr4WRyYnyz8VQsXJFXN77KoUuHjI70YDb8H8SchscnmRc5C2FjUhzd8eC6jITloQFN/HjEw5VPVkTY5xpYYbVyW4T1ADYDQ4C5SqmPlFItLRdLWBM3ZzemtpxKYZfCDFk7hDNxZ+7+Imt09k/4IwyCBkDZhkanEeK+JJkK4ibTkXmqgLOJ4a0rs+/MVX47cLfuS0LkndxeHblEa/068AKwAugH3LHBqrAvxQsWJ6xVGGk6jcFrB3Ml8YrRke6N1rDyTShUHFp+YHQaIe5bgoMb7uq6TEfmsafreFP1EXc+X3WUpFQ7bs0jrEpur45cqJQ6iXkzbjegD1DYksGE9SnvaW7mev7aeYaus7FmroeXQNRO896QBbyMTiPEfbvuYB4J83AxGR3FrpgcFG+1r8bpy9f5ccdpo+OIfCK305FjgMpa6zZa69Fa601aa9nrMR+qXaI2nwV/xsFLB3lj0xukptvAlh+pybDuIyheDQJ7GZ1GiAdyjQI4qnQ8nFKMjmJ3QioXp2mlYkxef5yYBPn+CsvLbREWDryolFqQcXtJKSVj4flUy7Itebv+22yM2sjHOz62/oWse2bA5VPQahQ4yOiBsG2xuiAAHlw3OIl9GtmuKjEJKUzdeMLoKCIfyG0RFgbUBaZm3OpkPCbyqW5Vu/F8zedZeHwh4/aMs94RscQY2DgG/IKhUiuj0wjxwGJ1AQBc064ZnMQ++Zf2pFPtMszYGknUFSl0hWXltgirp7Xuq7Ven3HrD9SzZDBh/V6q/RJdK3dl5qGZDFg9gLPxZ42OdLvfx0PCZWg12rzlixA27mqaKwAqWTq8W8qI1lUAGPc/2c5IWFaum7UqpSpk3lFKlcfcuFXkY0op3mv0HmOajuH4leN0XtqZ5aes6KLZmCjYEQYB3aB0oNFphMgTV9LMI2EkxhgbxI6V9irAgCZ+LA4/y8Gz8n0WlpPbIux1YINSaqNSaiOwHnjNYqmETelQvgMLHl9ApcKVeGvLW4zcMpK45DijY8H6T8ytKVq8a3QSIfLMxdSMJsNJscYGsXNDmlfAq4ATn66UBq7CcnJbhG3FvHdkesZtGrDdUqGE7SnjVobv23zPkMAhrPprFV2WdWHvv3uNC3T+AOz7GRq8AF5ljcshRB6LTs4swqzgg44d83B14uWWldh64hKbjkUbHUfYqdwWYbMAP2B0xs0PmG2pUMI2OTo4MrjWYGa2nQlAv1X9mBo+1ZhF+2veN/cDayoDtsK+XEh2Nv8mUUbCLK1Xg3KUK1qQMSuPkJ4uo2Ei7+W2CKuitX5Oa70h4zYQqGzJYMJ2BZYIZMFjC+jg14GwfWH0W9Xv4W51dGIdnFwPwa9LY1Zhd/5NzOgOJNORFufs6MDwVpU5cj6OFQdlOyOR93JbhO1VSmVttqeUaoB5ilKIbLk5u/F/Tf+Pz5p+xqmrp+iyrAvLTi6z/NqK9DTzKJhXOaj3nGXfSwgDxCalkehQUKYjH5KOAaWpXNKNcWuOkZqWbnQcYWdyW4Q1ALYppSKVUpGY14OFKKUOKKX2WyydsHnty7dnweMLqFK4Cm///jZdlnVh8t7J7I/eT7q2wA+0fXPhwkEI/QAcXfL+/EIYSGtNfFIqyY5uMh35kJgcFMNbVeZU9DWWhJ8zOo6wM465PK6tRVMIu1barTTft/meX479wqq/VjH9wHS+2f8NRVyLEOwdTDPvZjQq3YiCTgUf7I1SEmD9x1C6Dvg/lTfhhbAi15LTSNeQ6ugGSdI64WFp4/8I/qU9mLjuOI8HlsbJlNvxCyHuLFdFmNb67/s5uVKqLeZNv03AdK31mFueHwS8iLnnWDwwUGt9+H7eS1g3k4OJHlV70KNqD2KSYthydgubz2xm3d/r+PXErzg5OFH/kfrmosynGaXdSt/7m+yYCnHn4Onp0phV2KW4RPN+hmnO7jId+RAppRjRugr9Z+5iwZ4oetSXK65F3lCWWqOjlDIBx4BWQBSwC+hxY5GllPLQWsdm/P5xYIjW+o6jbkFBQXr37t0WySwevpT0FML/DWfjmY1sjtpMZGwkABW9KtLMpxkh3iHULFYT0932fLx2ESYGgl9T6PGz5YMLYYBjF+JoPX4zO8uFUcJ0DQZuMDpSvqG15umwbfwTk8iGEc1wdZJ9aEXuKKX2aK2Dsnsut9OR96M+cEJrfSojxFzgCSCrCMsswDIUAuQa4HzGycGJeo/Uo94j9Xi93utExkSyKWoTm6I2MePgDKYfmE5hl8I09W5KsHcwTUo3wc3Z7fYTbfocUq5D6EcP/4sQ4iHJHAnDxR2uydV6D5NSitdaV6HX9D+Yu/M0/Zr4GR1J2AFLFmFlgBv7EkRhXuB/E6XUi8BwwBlokd2JlFIDgYEAZcvKMLA98/X0xdfTl77+fYlNjmXb2W1ZRdnSk0txVI7UfaQuId4hhHiHUNajLFw6Cbu/gzp9oLh0ThH2KzbR3HPPwdUTLst05MPWuEJRGpYvwlcbTtKtXlkKOMtomHgwlizCsluUc9tIl9Z6CjBFKdUTeBfom80x3wDfgHk6Mo9zCivl4exBW7+2tPVrS2p6Kvuj95sLsjOb+HzX53y+63P8PP0IiY8nuEAhage/YdG/0EIYLS6jCDMV9JCrIw2QORrW5evtzNoeyQshFe76GiHuxJL/Z0UBPjfc9wbudH3vXCDMgnmEDXN0cKROyTrUKVmHV+u+ypm4M2yO2sym40v5MfUUM0t44r6iM4+WeZRm3s1oUqYJni6eRscWIk9lTkc6FfSC1ARISwGTk8Gp8pd6vkUIqVycrzedpGeDsri7yvdf3D9LXme7C6iklPJTSjkD3YGlNx6glKp0w90OwHEL5hF2xMfdh17VevHNlev8fjGJ8Y9+SgufFvzxzx+8ueVNQuaF0G9VP2YenMmpmFOyAa+wC5kjYc6FMj5gyBWShnitdWWuXE9hxtZIo6MIG2exkTCtdapSaiiwGnOLiu+11oeUUqOA3VrrpcBQpVQokAJcIZupSCFydGoT/LWZQm3HEFqhI6EVOpKu0zlw8QCbzpjXkX2550u+3PMlZd3LEuwdTIuyLahTos7dr7YUwgrFJqRgclDmkTCAxBgoWMTYUPlQgLcXrauX5NvNp+jTqBxeBZ2NjiRslMVaVFiKtKgQAGgN37eBq2fg5b3g5JrtYf/E/8PmqM1sjNrIzn92kpyeTIkCJWjr15b25dtTvUh1lPQUEzbivV8Psmz/OcK7JMG83vDCZihVy+hY+dKR87G0m7iFIc0q8HqbqkbHEVbMqBYVQljOibVw5g/oOD7HAgyglFspulXtRreq3biecp3NZzez4tQKfjryE7MOz8LXw5f25dvT3q895TzKPcQvQIh7F5eYgrurI7hkbMkl05GGqfqIBx0DSjNjayT9m/hRzE22SRP3TvZeELZHa1g/2rxJd2DvXL+soFNB2vq2ZVKLSWzsupEPG31IiYIlCAsPo+PijvRY3oPZh2cTfT3aguGFuH9xiam4uziBq4f5AblC0lDDQiuRmJLG1xtPGh1F2CgpwoTtObIc/tkHzUaC4/2txfB08eTpyk/zXZvvWNN5DSOCRpBOOp/v+pyW81vy3P+eY+nJpSSmJuZxeCHuX1xiasZIWEYRliRFmJEqFHfjqTrezN7xNxdi5WeFuHdShAnbkp4G6z+BopWgZtc8OWXJQiXp69+XeR3nsfTJpQyqNYh/4v/hnd/foeX8lny28zNOxZzKk/cS4kHEJqbgUcDphiJMpiON9krLSqSla6ZsOGF0FGGDpAgTtuXQYoiOgOZvgSnvlzT6efoxJHAIyzst5/s239O4dGPmHp3LE78+wYDVA1j510qS05Lz/H2FyI2skbCs6cgYYwMJfIoUpGs9H37eeZqoK9eNjiNsjBRhwnakpcKG/4MS/lC9k0XfSilFvUfq8UXIF6ztvJZhdYZxLv4cb2x+g1YLWjFuzzjOxJ65+4mEyENxiSl4uDqBowuYnGU60kq81KIiSikmr5PRMHFvpAgTtmP/XLh8Elq8Aw4P769u0QJFebbms6x4agVfh35N7RK1mXVoFu0Xt+eFNS+w9u+1pKSnPLQ8In/SWhOflDESBuYpSZmOtAqlPAvQq0FZFvwZxV8XrxkdR9gQKcKEbUhNho2fQenaUKW9IREclANNyjRhQvMJrH56NUMCh3Dy6kle3fgqbRe0ZUr4FM5fO29INmH/riWnka75rwhzlf0jrcmQZhVxNjnw5f+OGh1F2BApwoRt2DsLYk5D83fBCpqrlixUksG1BrPq6VVMaj6JykUqM23fNNosbMPL61/m97O/k67TjY4p7EjmvpFZexW6uMt0pBUp7u7C8039WL7/H8LPXDU6jrARUoQJ65eSAJvHgk9DqNjS6DQ3cXRwpHnZ5oSFhrHiqRX09+/Pvuh9DF47mPaL2jP9wHQuJVwyOqawA5n7Rsp0pPUaGFKBYm7O/N+KCNmvVuSKFGHC+u3+HuL+gRbWMQqWE293b4bVHcbazmv5IvgLShUqxcQ/JxK6IJQ3Nr3BrvO75AezuG+3jYS5esp0pJVxc3HkldDK7PzrMmsj/jU6jrABsm2RsG5J8bBlHPiFgF9To9PkipPJibZ+bWnr15ZTV08x/9h8lpxcwsrIlZT3LE/XKl3pWL4jni6eRkcVNiT2tpEwmY60Rt3r+TBj61+MWRlB8yrFcTTJWIfImfztENZt5zS4ftE8CmaDynuV5836b7KuyzpGNR5FQceCjNk5hpbzW/Lu7++yP3q/jI6JXIlNMI+Eedw0HSlFmLVxMjkwsm1VTkZfY95uaWMj7kxGwoT1SoyBrZOgUhvwqW90mgdSwLEAnSp1olOlTkRcimD+sfksP7WcJSeXULVIVbpU7kKH8h0o5FTI6KjCSv23JixzOjJjTZjWVj1Nnx+1ql6S+r5FGL/mOE8ElsHNRf6rFdmTkTBhvbZPhcSr0Pxto5PkqWpFq/F+o/dZ32U97zV8D601o3eMpsUvLRi1fRQRlyKMjiis0O0L891Bp0Oy9KWyNkop3mpflYvxSXyzWbY8EzmTIkxYp+uXYfsUqPY4lA40Oo1FuDm70bVKV+Y/Np857efQqlwrlp5cStflXen5W08WH18sG4iLLHGJKZgcFAWcTOYHZBNvq1a7bGE6BJTi282nZHNvkSMpwoR12joRkuPtbhQsO0opAooH8PGjH7OuyzpG1h/JtZRrvL/tfVrOb8nYXWNliyRBXGIqHq6OqMypx6z9I6UIs1ZvtqlKano6E9YeMzqKsFIWLcKUUm2VUkeVUieUUiOzeX64UuqwUmq/UmqdUqqcJfMIGxF3HnZ+AzU7Q4lqRqd5qDxdPOlVrRe/PvEr37f5noalGjInYg7tF7dn0NpBbDqzibT0NKNjCgPEJab8tx4MbhgJk15h1qps0YI809CXebvOcOyC/DmJ21msCFNKmYApQDugOtBDKVX9lsP2AkFa6wBgAfC5pfIIG7LmA0hPhWZvGZ3EMJkbiH/Z7EtWd17NkFpDOHb5GEPXD6XD4g58f/B7riReMTqmeIjiEm/YNxJuKMJijAkkcuWlFhUp5OLImJVHjI4irJAlR8LqAye01qe01snAXOCJGw/QWm/QWl/PuLsD8LZgHmELTv9h3qi70VAoWsHoNFahRMESDA4czOrOqxkbMpZShUoxfs94QueH8s7v73Ag+oDREcVDcFsRJtORNqFwIWdebF6R9Uf+ZdvJi0bHEVbGkkVYGeDGhSxRGY/l5FlgpQXzCGuXngYrRoB7aWj6mtFprI6TgxNtfNswo+0MFj2+iE6VOrH277X0XNGT7su7s+zkMpLTko2OKSwk9rbpSHfzrzIdafX6NfaljFcBPl1xhPR06Qso/mPJIiy7xjXZ/u1TSvUGgoAvcnh+oFJqt1Jqd3R0dB5GFFblz1lwfj+0Hg0ubkansWqVClfi3Ybvsq7LOt5u8DbXUq7x9u9v03pBa8LCw7iYIJ+47U3O05EyEmbtXJ1MvNa6MgfOxrBs/zmj4wgrYskiLArwueG+N3Db3z6lVCjwDvC41jopuxNprb/RWgdprYOKFy9ukbDCYNcvw7pRUK4J1Hja6DQ2w83ZjR5Ve7DkySV8Hfo11YtWZ+q+qbRa0Iq3t7zNoUuHjI4o8khcYgoeN46EObsBSqYjbcSTgWXwL+3B56uOkpgiF9cIM0sWYbuASkopP6WUM9AdWHrjAUqp2sA0zAWY7Haan234P3Nj1nafSffv++CgHGhSpglTQ6ey7MlldK3clXWn19F9eXeeWfEMqyJXkZKeYnRMcZ+01sQn3TIS5uCQsX+kTEfaAgcHxdvtq3H2agKzt/9tdBxhJSxWhGmtU4GhwGogAvhFa31IKTVKKfV4xmFfAG7AfKVUuFJqaQ6nE/bs/EHY/R0EPQuP1DQ6jc3z9fTlrQZvsbbLWt6s9yaXEi/x+qbXabewHdMPTJerKm3QteQ00jU3F2Eg+0famCYVi9GsSnEmrz/O1euyflNYuE+Y1nqF1rqy1rqC1vqTjMfe11ovzfh9qNa6pNY6MOP2+J3PKOyO1rDyDXD1yheNWR8md2d3elfvzbInl/FVi6/w8/Rj4p8TabWgFR9s+4Cjl48aHVHkUubm3TctzAfzFZKJ0qLCloxsV5X4pFS+Wn/C6CjCCsiuosJYBxfC31uh43goWMToNHbJ5GAixCeEEJ8QTlw5wc9HfmbZqWUsOr6IuiXr0qtaL5r7NMfRQX4cWKvb9o3MJNORNqfqIx50ruvNrO1/06eRL2WLFjQ6kjCQbFskjJMUD/97D0rVgjp9jU6TL1QsXJH3Gr3Hms5rGBE0gvPXzjN843DaLWrHdwe+42riVaMjimzEJeYwEibTkTbptdZVcDIp3ltyEK2lDo74RgAAHHZJREFUZUV+JkWYMM7v4yDuHLT7AhxMRqfJVzxdPOnr35ffOv3GxOYTKedejgl/TiB0QSgfbvtQpiqtTOZImEd2I2FydaTNKenhyuttqrDpWDRL90nLivxM5h+EMS6dhG2TIaA7lG1gdJp8y+RgokXZFrQo24LjV47z05GfWH5yOQuPLySoZFDWVKVJimRDxeY0EubqIdORNuqZRr78Gn6OUcsOE1ypOIULORsdSRhARsKEMVa/DSZnaPWR0UlEhkqFK/FBow9Y22Utw+sO52z8WV7d+CrtF7Vn5sGZxMgehYbJeSRMpiNtlclB8elTNYlJSOH/VkQYHUcYRIow8fAd+x8cWwUhb4D7I0anEbfwdPGkf43+rHhqBeObjae0W2m+3PMlrRa0YvT20Zy8etLoiPnOfwvzs1kTlpoIqdLuwBZVK+XBwODyzN8TxbYTsstFfiRFmHi4UpNg1UgoWgkaDDY6jbgDRwdHQsuFMqPtDOY/Nv//27vz+KiqbNHjv5XKRCAEkgABAoQhiCYyJ6EFUQYRxAeIgArS2CgoLQ7Xtr106+X1g+Ze0aetH7UVRBwaBaEBQRkEZFJmZJCAoCGJEAkCATJAQqZ9/6hKKEISQkjlVCrr+/kUderUqXNWLaBq1d7n7M3AiIF8kfAFw5YNY+KaiWw6volCU2h1mLVCZk4e3l6Cv0+Jj+yiSby1S7LGerpfJBEhAfx16QEdSb8W0iJMVa/t/4SzR2HQy+Ct50DUFB2COzCt5zTWjlzL012e5uj5o0xeP5l7l97LvEPzyMrNsjpEj1Y0b6SUnE2ieP5I7Squqfx9bPz3fbeSnHaRt9b/bHU4qpppEaaqT8YJ2PQq3DQY2vW3OhpVCcH+wUzoOIHVI1bzau9XCfYPZuaumfRb1I/p26brVZUukpmTd3VXJNivjgS9QrKGu61dKCO6hTNrUyI/purfZW2iRZiqPmunQmE+3D3D6kjUDfLx8mFg64HMu2ce8wfPp3+r/nyR8AUjvhzBmJVjWJawjJz8HKvD9BhFLWFX0e5Ij/HiPTcTVMeHvyw5QEGhjh1WW2gRpqrHkdVwYBH0fAaCW1sdjapC0aHRzOg1g/Wj1vNCzAtkXMrgpS0v0XdRX2bunEni+USrQ6zxyizCirsjtfWkpmtY15ep/+cW9h0/z7ztOsF3baFFmHK9C2mw/CloEg29n7c6GuUiQX5BjL1lLMuHLWfu3XPp2awnC44sYOiyofxh9R9YlbSK3AK9iq8yMrQ7slYY0qkZvds34pXVhzlxPtvqcFQ10CJMuZYx8NWzkH0O7psF3n5WR6RcTESICYvh1TteZd2IdTzb9VlSL6TywuYX6L+oP6/vfl1bx65T2d2RQfZ77Y70CCLCjGHRFBjDVJ3SqFbQIky51oFF8ONy6PsihEVbHY2qZiF1Qnj01kdZOXwl7/V/j65NuvLJoU8YumwoY1aMYeGRhWTkaivOtWTk5FG/vJYwvTrSY7QIDuC5u9qz7sdTrI4/aXU4ysW0CFOuk/4rrHgeWsTBbU9bHY2ykJd40bN5T97o8wbrRq7j+e7PczH/ItO3T6fP5314YdMLbPl1CwWFOk5SSYWFhqxL+VePlg/2lmWbn3ZHepjxPVsT1aw+U5cfJD07z+pwlAtpEaZcwxhY9qT9asj73tMJulWx0DqhjIsax5IhS1hw7wLub38/W1O38sS6JxiweABv7nmTpPQkq8N0Gxdy8zGmlNHyi+j8kR7H2+bFy8M7kpZ1iZmrD1sdjnIhLcKUa+yaA4kb4O6/Q3Abq6NRbkhEiAqJ4q9xf2X9yPW8fufrdAjuwIfxHzLkiyE8vPJhFv20iMzc2l1gXJ6yqJSWMND5Iz3UreFBjO/Zms92HGNX8lmrw1EuokWYqnppR2HNf9kHZO32B6ujUTWAr82Xu1rdxTv93mHtiLX8qdufyMrNYtq2afRd2Jcp305he+r2WjlNUpnzRhbxC9TuSA/13ID2NG9QhymLfyDrUr7V4SgXcGkRJiIDReSIiCSIyJRSnu8tIntEJF9ERrgyFlVNCvJh6eP2c1WGvAUlp1lR6hoaBTTikehHWDp0KfMHz2dou6FsTtnMhDUTGLR4EO/se4eUzBSrw6w2mTn2c4LKbAnT7kiPFeDrzcz7O5KcdpGxH+zQ88M8kMuKMBGxAe8Ag4BbgIdE5JYSmx0DHgE+c1UcqppteQNSdsHg16B+M6ujUTWYiBAdGs1LPV5iw6gNvNL7FSKCIpi1fxaDlgxi/NfjWZawjIt5F60O1aW0O7J26xUZyjujuxL/azpj5mzn3AUda8+TuLIlLBZIMMYkGmNygQXAUOcNjDHJxpgfgNrXx+CJUvfDxv+BqOFwqzZsqqrjZ/NjUOtBzLprFmtGrOGpLk/x24XfeGnLS/RZ2IepW6ay57c9HjmuUkZxS1hZ3ZH1tTvSww2MDmPW2G789FsWD72/nTNZl6wOSVURVxZhzYHjTo9THOuum4hMFJHdIrL79OnTVRKcqmJ5ObD0CQgItbeCKeUiYXXDmNhxIl/d9xUfDfyIAREDWJ28mnGrx3Hv0nuZ/cNsUrNSrQ6zyhS1hJU6RAVod2Qt0bdDEz4Y153ktAs8OHs7pzJ0blZP4MoirLSTgSr1M9UYM9sY090Y071Ro0Y3GJZyiQ0z4NQhGPo2BARbHY2qBUSEbk26Mb3ndDaO2siMXjNoUrcJb+19i7sX382ENRNYkbiC7PyaPf3LtU/Md3RHFmqHgqe7PbIRH/0hlhPns3lg9nZS02v2v23l2iIsBWjh9DgcOOHC4ymr/LIVtr4F3R6ByLusjkbVQgE+AQxpO4S5d89l1fBVTOo0ieOZx5ny7RT6LuzL37b+jX2n9tXI7srMnDy8vQR/nzI+rv0CAQO5WdUal7JGjzYh/OvRWM5kXmLUrG0cP+vZ50R6OlcWYbuASBFpLSK+wIPAchceT1nhUqa9G7JhKxgww+polCI8MJxJnSexcvhK5t49l74t+7IyaSVjV41lyBdDmHNgDsczj197R26iaN5IKetKY//69nvtkqw1urUKZt5jcaRfzOOBWdtIPnPB6pBUJbmsCDPG5AOTga+BH4GFxpiDIjJNRIYAiEiMiKQAI4FZInLQVfEoFygshBV/gvPHYNh74FfP6oiUKuYlXsSExTCj1ww2jNrAtNumEVInhDf3vMk9S+7h/uX38+6+dzly9ohbt5Bl5uSV3RUJ9u5I0Cska5lOLRowf2IPsvMKGDVrGwmntCW0JhJ3/vApTffu3c3u3butDkMVFsCyybD/M+jzEtzxZ6sjUqpCUjJT+ObYN6w/tp69p/ZiMITXC6d/q/70a9mPjo064iXuM471ox/t4mRGDiuevr30DX5eB5/eD+PXQMu46g1OWe7IyUzGzNkBGOY9FkeHsPpWh6RKEJHvjTHdS3vOfT5pVM1RkAdLJjgKsBe1AFM1SnhgOOOixvHxoI9ZP2o9U383lVZBrZj34zzGrhpL/0X9mb5tOlt/3UpegfWDY2bk5FG/vJYw7Y6s1W4KC+Tzx3tg8xIemr2d+F/TrQ5JXYcyrnlWqgz5ubB4PPz4JfT/f9DrWasjUqrSQuuEMrL9SEa2H0lmbiabUzbzzbFv+DLxSxb+tJBAn0BiwmKIbRpLj6Y9aBPUpuxzs1wkMyeflsEBZW9Q3B2pX761VdtG9Vj4+O8Y/f4OHpi1jenDohneNdzqsFQFaBGmKi4vBxb+Hn7+GgbOhB5PWB2RUlUm0DeQwW0GM7jNYHLyc9h2YhsbUzayI3UH64+vByDEP6S4IIsNiyU80PVfdPYT88s7JyzQfq8tYbVaq5C6/HvS73hmwT6eW7ifb38+w/Rh0dTz0695d6Z/O6pici/C52Pg6Hq49x/QfbzVESnlMv7e/vRp2Yc+LfsA9vPIdp7cyY7UHew8uZNVSasAaF6vOXFN44gNiyWuaRyhdUKrPJaMnLyypyyCy92ROmp+rdc0qA7zJ/Tg7fUJvPnNT+w5do63HupCx/AGVoemyqBFmLq2S1kw/0FI/g6G/hO6jLE6IqWqVXhgOOGB4QyPHI4xhsT0RHak7mBH6g7W/rKWJT8vAaBNUJvigiwmLIYgv6AbOm5hoSHrUn7Zo+UD+NYDRK+OVADYvIRn+kdyW7sQnpm/l+H/3MoLA2/isV5t8PKq3q50dW1ahKny5WTApyPtk3IPfx86jrQ6IqUsJSK0bdCWtg3aMvrm0RQUFnD43OHiVrJlR5ex4MgCBKFDcAdiw2KJbRpLtybdqOtT97qOdSE3H2PKGS3fHpBj1HztjlSXxUQEs/KZ25my+AD/vfIw3yWk8drITjQK9LM6NOVEizBVtuxzMO9++8TcI+ZC1DCrI1LK7di8bESFRBEVEsX46PHkFeQRnxZfXJR9dvgzPj70MTaxERUaRVxYHLFNY+ncqDP+3v7l7vvylEXX+Kj210m81dUaBPjy7sNd+WznMaZ9eYhBb27mtVGduaO9Tv/nLrQIU6W7kAb/GganD8MD8+CmQVZHpFSN4GPzoUvjLnRp3IUnOj1BTn4O+07vY2fqTnae3Mnc+Lm8f+B9fL186dy4c/E5ZdGh0Xh7XfmRfM15I4sUzR+pVAkiwpi4VnRvFcxT8/cwbu5OJvZuw/MDbsLXW0epspoWYepqmSfhX/fB2UR4cD5E9rc6IqVqLH9vf3o07UGPpj0AyMrNYs+pPcUtZW/tfQuAuj516dakG3FhccQ1jSOyYSSZOfZxyq7ZEuYXqEWYKtdNYYEsn9yLv684xOzNiWxPTOP1UZ1p11hnOrGSFmHqssIC+P5D+GaafUDW0QuhzR1WR6WUR6nnW4/e4b3pHd4bgHM559h1cldxUbY5ZTMADf0a0qpuR3waBnLyUiA5+YFld1/614esU9X1FlQN5e9j4+/DbqVXu0b85+IfGPCPTQzp1Iwn+7Qjskmg1eHVSjptkbL7dQ+seA5O7IWI22Hwa9DoJqujUqrWOXnhZPFwGBuPbSUj7wwANrHRrkE7okOjiQq1n4MW2TASHy8f+Pd4+//dp/daHL2qKU5nXuL9bxOZt/0XsvMKGBgVxpN92hHd/Mau6FVXK2/aIi3CaruLZ2H9dNj9IdRrDANmwK0j7FdcKaUsNW/7L/zXV1t5/eFgjl88wsG0gxxMO0i6Y3R8Xy9fOgR3ICrzLFGnE2k3egmt67cmwKecEfaVcnL2Qi4fbknioy3JZF7Kp1+HxjzZtx1dWza0OjSPoUWYulphoX3ux7VT7VdBxj4Off4C/vorSCl38e7Go8xcfZjD0wfi72MDwBhDSlYKB88cJP5MPPFp8Rw6tY9sU1D8uiYBTYgIiqB1/da0Drp8axLQpNqnXVI1Q3p2Hp9sTeaDLUmcv5hHr3ahTO7bjrjWwfpv5gZpEaaudDIeVvwJjm+HFnH2rsewW62OSilVwszVh5nzbSI//X1QuV+EBZtm8st3r5D40DySslJIzkgmKT2JpPQksvKyirer412HiPoRxQVaRFAEEfUjaFW/lbaeKQAuXMrn0x2/MHtzEmeyLhET0ZDJfSPpHRmqxVgllVeE6Yn5tUlOBmz8H9gxC+o0gKHvQKfR4KWXKSvljjJz8gj097nml5/NvyFt8vJp0yQG2lweTsYYQ1pOWnFBlpSeRFJGEj+c/oHVSasxXP4RHlY3zF6glSjSwuqG4SX6GVFb1PXzZmLvtvz+dxF8vus47206yri5O+kUHsTkvpH0v7mxFmNVSIuw2uDcL/DDQtg1B7J+g26PQL+pEBBsdWRKqXLYJ++uwMd08fyR6VD38vyVIkJonVBC64QSExZzxUty8nM4lnmM5HR7q1lyRjLJ6cl8lfjVFa1n/jZ/WtVvdUW3praeeT5/HxvjbovgwdgWLP7+V97dlMCET3bTISyQyX3bMSi6KTadBumGaRHmqbLPw6FlsH8BHNtqXxdxOzz0GTTvZm1sSqkKqXAR5ucYXuA6xgrz9/anfcP2tG/Y/or1pbWeJWckE38mnq+Tv76i9axp3abFRVnroNY0r9eckDohhPiHEFwn2H7lpqrR/LxtjI5ryaju4Szff4K3NyQw+bO9tG30E0/2aceQTs3wtmlLaWW5tAgTkYHAm4ANmGOMebnE837AJ0A3IA14wBiT7MqYPFpBHiSssxdeR1ZBwSUIiYS+L8Gto6BhK6sjVEpdh8ycPAL9KlDI+Dlawqpg/siKtJ45F2dJ6UksTVhKdn72VfsK8gsixD+kuDBzvg/wDsDH5oOvly++NsfNy7d4nfNzPl4++Np8r5pRQFUfb5sXw7uGM7Rzc1bFp/L2+gSeW7ifN9b9zB/vbMvwruE6An8luOxftIjYgHeAu4AUYJeILDfGHHLa7FHgnDGmnYg8CMwEHnBVTB7JGDixB/Z/DvGL4eIZCAixdzl2egCaddXhJpSqoTJz8mkZXIEuv+LuSNeOml9e69lvF3/j5IWTpGWnkZaTdtX9obRDpOWkcSHvQqWP7yVe9oKsqEgrKtCKijWbzxWPnQu4kgWej5cP3l7e2MRmv3mVuHcse4t38TlQguNe5PIygmMRQUrfxukzuMx92Beueo3zNmXtp+TzhaYQY0xxq6UxhkKc1hmuaNG8Fuf9GAz1g2HKcMO+lHy+2n+YF9fs5rXvfBkYHUavyBC8bVJ8jEJTiOHK4xavc7ow0Pmx8/HKer7UbZzfc4mYr3g/Tuu6Nu5KVGhUhXNR1Vz5syIWSDDGJAKIyAJgKOBchA0F/uZY/jfwtoiIsfCSzfhvl1Fvw4tWHf66BZhsGpsz5OLDNu9Y1taZxG5bFwoOe8PhLGCz1SEqpSopOe0CUc0qMGxMUXfkyj/bx/2rZgKEOW7XkkMd0sSQLZAL5GLIA/LEvpwL5Ar2dRhyi7eDXDHkU0Au2eRKdpmvvQCcd3qc67SvPMe+jP42rRoBEBAAOcAXqfZbTTLCN4aoh+ZadnxXFmHNgeNOj1OAuLK2Mcbki0g6EAKccd5IRCYCEwFatmzpqngB8K0bxNmA1i49RlU6jY01AV3ZW+8Osm32OcDaWByTUqpqtG8SyMju4dfesEEr+1h/WSddH9QN8sf+wW8lYwz5QCGGAgwF4Lh3LBtT/Djf0WZS1DLgvOzcnlRyG+ctSq432DsxnNeW9vrL985HLP1552MKFLfJSZmPKf6zIuRyY13xq4r2WyQ9O49T6TnFrV7Ox3R+rTi9SpwCd9728jq5Yl1pywLFySi5fdn7tfON7l3yrVYrVxZhpf3tlmzhqsg2GGNmA7PBPk7YjYdWtvZd74Sud7ryEFUuBnjY6iCUUtbxssE9r1gdRY0hgF4yoNyBK8+iSwFaOD0OB06UtY2IeANBwFkXxqSUUkop5RZcWYTtAiJFpLWI+AIPAstLbLMcGOdYHgGst/J8MKWUUkqp6uKy7kjHOV6Tga+xD1Ex1xhzUESmAbuNMcuBD4B/iUgC9hawB10Vj1JKKaWUO3HpoCvGmJXAyhLrpjot5wAjXRmDUkoppZQ70pHVlFJKKaUsoEWYUkoppZQFtAhTSimllLKAFmFKKaWUUhaQmjYihIicBn6xOo5qEEqJmQNUldC8uo7m1jU0r66heXUdze2VWhljGpX2RI0rwmoLEdltjOludRyeRvPqOppb19C8uobm1XU0txWn3ZFKKaWUUhbQIkwppZRSygJahLmv2VYH4KE0r66juXUNzatraF5dR3NbQXpOmFJKKaWUBbQlTCmllFLKAlqEKaWUUkpZQIuwKiAiA0XkiIgkiMgUp/WTHeuMiISW8/pPHa+PF5G5IuJT4vkYESkQkRHXefx+IrJHRPaJyHci0q4q3m91cYO8zhWRUyISX2J9sIisFZGfHfcNb/S9Vic3zuurInJYRH4QkaUi0uBG32t1c9fcOj3//LVicEfunFcRecqx74Mi8sqNvM/q5q55FZHOIrLd8d21W0Rib/S9ui1jjN5u4AbYgKNAG8AX2A/c4niuCxABJAOh5ezjHkAct/nApBL7Xw+sBEZc5/F/Am52LP8R+MjqfNWUvDq26Q10BeJLrH8FmOJYngLMtDpfHpLXAYC3Y3lmTcqru+fW8VwL4Gvsg12XGYO73dw5r0AfYB3g53jc2Op8eUhe1wCDnI6x0ep8ueqmLWE3LhZIMMYkGmNygQXAUABjzF5jTPK1dmCMWWkcgJ1AuNPTTwGLgVPXe3zAAPUdy0HAiet6Z9ayOq8YYzYDZ0t5aijwsWP5Y2DYtd+O23DbvBpj1hhj8h0Pt5fYb03gtrl1+AfwAvbPhZrEnfM6CXjZGHPJsV2Z+3BD7pzXmvzddV20CLtxzYHjTo9THOuum6Mpdyyw2vG4OXAf8F4lj/8YsFJEUhz7fbkycVnE6ryWp4kxJhXAcd+4kvuxgjvn1dl4YFUV7Kc6uW1uRWQI8KsxZn9lXm8xt80r0B64XUR2iMgmEYmp5H6s4M55fRZ4VUSOA/8f+Esl9+P2tAi7cVLKusr+0vwnsNkY863j8RvAfxpjCip5/P8A7jHGhAMfAq9XMi4rWJ1XT+X2eRWRF4F84NMb2Y8F3DK3IhIAvAhMrWQsVnPLvDp4Aw2BHsCfgYUiUlq87sid8zoJ+A9jTAvs32MfVHI/bs/b6gA8QAr2cy2KhHONplMR+RpoAuw2xjzmWPd/gUbA406bdgcWOP5PhwL3iEi+MeaLax1fRBoBnYwxOxzrP8fxK6WGsDqv5flNRJoaY1JFpCnlNLe7IXfOKyIyDrgX6Ofo4qhJ3DW3bYHWwH7H68OBPSISa4w5WZE3ZjF3zWtRbEuKuuNEpNCxn9MVfL2V3Dmv44BnHMuLgDkVfF3NcyMnlOnNgL2QTcT+IVd0cmNUiW2SKf/kxseArUCdcrb5iNJPzC/1+I71Z4D2ju0eBRZbna+aklen5yO4+qTRV7nyxPxXrM6Xh+R1IHAIaGR1njwtt9cTg7vd3DmvwBPANMdye+zde2J1zjwgrz8CdzqW+wHfW50vl/09WB2AJ9ywX73xE/YrTV50Wv809l8b+dh/Ycwp4/X5jtfuc9ymlrJNmf+Qyzn+fcABx3+ujUAbq3NVw/I6H0gF8hzHe9SxPgT4BvjZcR9sda48JK8Jji+xov2+Z3WuPCW3JbZJpgYVYe6cV+zFyzwgHtgD9LU6Vx6S117A99i/u3YA3azOlatuOm2RUkoppZQF9MR8pZRSSikLaBGmlFJKKWUBLcKUUkoppSygRZhSSimllAW0CFNKKaWUsoAWYUopjyUiDUTkj47lZiLyb6tjUkqpIjpEhVLKY4lIBPCVMSba4lCUUuoqOm2RUsqTvQy0FZF92AfXvdkYEy0ijwDDABsQDbyGfeDNscAl7HOunhWRtsA72KdluQhMMMYcrv63oZTyRNodqZTyZFOAo8aYztgnWHYWDYwGYoEZwEVjTBdgG/B7xzazgaeMMd2A57FPVKyUUlVCW8KUUrXVBmNMJpApIunAl471B4COIlIPuA1Y5JiIGMCv+sNUSnkqLcKUUrXVJaflQqfHhdg/G72A845WNKWUqnLaHamU8mSZQGBlXmiMyQCSRGQkgNh1qsrglFK1mxZhSimPZYxJA7aISDzwaiV2MQZ4VET2AweBoVUZn1KqdtMhKpRSSimlLKAtYUoppZRSFtAiTCmllFLKAlqEKaWUUkpZQIswpZRSSikLaBGmlFJKKWUBLcKUUkoppSygRZhSSimllAX+Fx9RKt2bTMyIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 23\n",
    "graph = plt.figure(figsize=(10, 4))\n",
    "ax = graph.add_subplot(111)\n",
    "plt.plot(power_observe['stable'][48*(k-1):48*k]/20, label='power_stable')\n",
    "plt.plot(power_observe['fluc'][48*(k-1):48*k]/20, label='power_fluctuate')\n",
    "plt.plot(cloud.cloud_amount.loc[power_66[48*(k-1):48*k].index]/100, label='cloud')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('power')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建波动小情况下的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = power_66.index\n",
    "scaler1 = MinMaxScaler(feature_range=(0,1))\n",
    "power_66_normalized = pd.DataFrame(scaler1.fit_transform(power_66))\n",
    "power_66_normalized['index'] = index\n",
    "power_66_normalized = power_66_normalized.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>var1(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var1(t+1)</th>\n",
       "      <th>var1(t+2)</th>\n",
       "      <th>var1(t+3)</th>\n",
       "      <th>var1(t+4)</th>\n",
       "      <th>var1(t+5)</th>\n",
       "      <th>var1(t+6)</th>\n",
       "      <th>var1(t+7)</th>\n",
       "      <th>var1(t+8)</th>\n",
       "      <th>var1(t+9)</th>\n",
       "      <th>var1(t+10)</th>\n",
       "      <th>var1(t+11)</th>\n",
       "      <th>var1(t+12)</th>\n",
       "      <th>var1(t+13)</th>\n",
       "      <th>var1(t+14)</th>\n",
       "      <th>var1(t+15)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2017-01-04 16:00:00</td>\n",
       "      <td>0.494808</td>\n",
       "      <td>0.432932</td>\n",
       "      <td>0.373207</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.180226</td>\n",
       "      <td>0.127954</td>\n",
       "      <td>0.087437</td>\n",
       "      <td>0.057574</td>\n",
       "      <td>0.031963</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-04 16:15:00</td>\n",
       "      <td>0.432932</td>\n",
       "      <td>0.373207</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.180226</td>\n",
       "      <td>0.127954</td>\n",
       "      <td>0.087437</td>\n",
       "      <td>0.057574</td>\n",
       "      <td>0.031963</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-04 16:30:00</td>\n",
       "      <td>0.373207</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.180226</td>\n",
       "      <td>0.127954</td>\n",
       "      <td>0.087437</td>\n",
       "      <td>0.057574</td>\n",
       "      <td>0.031963</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-04 16:45:00</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.180226</td>\n",
       "      <td>0.127954</td>\n",
       "      <td>0.087437</td>\n",
       "      <td>0.057574</td>\n",
       "      <td>0.031963</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     var1(t-4)  var1(t-3)  var1(t-2)  var1(t-1)   var1(t)  \\\n",
       "index                                                                       \n",
       "2017-01-04 16:00:00   0.494808   0.432932   0.373207   0.323086  0.247404   \n",
       "2017-01-04 16:15:00   0.432932   0.373207   0.323086   0.247404  0.180226   \n",
       "2017-01-04 16:30:00   0.373207   0.323086   0.247404   0.180226  0.127954   \n",
       "2017-01-04 16:45:00   0.323086   0.247404   0.180226   0.127954  0.087437   \n",
       "\n",
       "                     var1(t+1)  var1(t+2)  var1(t+3)  var1(t+4)  var1(t+5)  \\\n",
       "index                                                                        \n",
       "2017-01-04 16:00:00   0.180226   0.127954   0.087437   0.057574   0.031963   \n",
       "2017-01-04 16:15:00   0.127954   0.087437   0.057574   0.031963   0.010654   \n",
       "2017-01-04 16:30:00   0.087437   0.057574   0.031963   0.010654   0.000000   \n",
       "2017-01-04 16:45:00   0.057574   0.031963   0.010654   0.000000   0.000000   \n",
       "\n",
       "                     var1(t+6)  var1(t+7)  var1(t+8)  var1(t+9)  var1(t+10)  \\\n",
       "index                                                                         \n",
       "2017-01-04 16:00:00   0.010654        0.0        0.0        0.0         0.0   \n",
       "2017-01-04 16:15:00   0.000000        0.0        0.0        0.0         0.0   \n",
       "2017-01-04 16:30:00   0.000000        0.0        0.0        0.0         0.0   \n",
       "2017-01-04 16:45:00   0.000000        0.0        0.0        0.0         0.0   \n",
       "\n",
       "                     var1(t+11)  var1(t+12)  var1(t+13)  var1(t+14)  \\\n",
       "index                                                                 \n",
       "2017-01-04 16:00:00         0.0         0.0         0.0         0.0   \n",
       "2017-01-04 16:15:00         0.0         0.0         0.0         0.0   \n",
       "2017-01-04 16:30:00         0.0         0.0         0.0         0.0   \n",
       "2017-01-04 16:45:00         0.0         0.0         0.0         0.0   \n",
       "\n",
       "                     var1(t+15)  \n",
       "index                            \n",
       "2017-01-04 16:00:00    0.000000  \n",
       "2017-01-04 16:15:00    0.000000  \n",
       "2017-01-04 16:30:00    0.000000  \n",
       "2017-01-04 16:45:00    0.014906  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_66_supervised = series_to_supervised(power_66_normalized, 48, 16)\n",
    "power_66_supervised.iloc[83:87,-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_inputs = list(range(16))+list(range(0,-8,-1))\n",
    "inputs = power_66_supervised.iloc[:,:48]\n",
    "outputs = power_66_supervised.iloc[:,48:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    names['output_%s' % str(i+1)] = outputs.iloc[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayuan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    names['output_stable_%s' % str(i+1)] = names['output_%s' % str(i+1)].loc[list(map(lambda x:x+datetime.timedelta(minutes=15*i), index_stable))]\n",
    "    names['output_stable_%s' % str(i+1)] = names['output_stable_%s' % str(i+1)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    names['input_stable_%s' % str(i+1)] = inputs.loc[names['output_stable_%s' % str(i+1)].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-48)</th>\n",
       "      <th>var1(t-47)</th>\n",
       "      <th>var1(t-46)</th>\n",
       "      <th>var1(t-45)</th>\n",
       "      <th>var1(t-44)</th>\n",
       "      <th>var1(t-43)</th>\n",
       "      <th>var1(t-42)</th>\n",
       "      <th>var1(t-41)</th>\n",
       "      <th>var1(t-40)</th>\n",
       "      <th>var1(t-39)</th>\n",
       "      <th>...</th>\n",
       "      <th>var1(t-10)</th>\n",
       "      <th>var1(t-9)</th>\n",
       "      <th>var1(t-8)</th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>var1(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2017-01-04 16:00:00</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.221793</td>\n",
       "      <td>0.136457</td>\n",
       "      <td>0.086336</td>\n",
       "      <td>0.055423</td>\n",
       "      <td>0.030913</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693141</td>\n",
       "      <td>0.679285</td>\n",
       "      <td>0.664379</td>\n",
       "      <td>0.634516</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.549181</td>\n",
       "      <td>0.494808</td>\n",
       "      <td>0.432932</td>\n",
       "      <td>0.373207</td>\n",
       "      <td>0.323086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-04 16:15:00</td>\n",
       "      <td>0.221793</td>\n",
       "      <td>0.136457</td>\n",
       "      <td>0.086336</td>\n",
       "      <td>0.055423</td>\n",
       "      <td>0.030913</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679285</td>\n",
       "      <td>0.664379</td>\n",
       "      <td>0.634516</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.549181</td>\n",
       "      <td>0.494808</td>\n",
       "      <td>0.432932</td>\n",
       "      <td>0.373207</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.247404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-04 16:30:00</td>\n",
       "      <td>0.136457</td>\n",
       "      <td>0.086336</td>\n",
       "      <td>0.055423</td>\n",
       "      <td>0.030913</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664379</td>\n",
       "      <td>0.634516</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.549181</td>\n",
       "      <td>0.494808</td>\n",
       "      <td>0.432932</td>\n",
       "      <td>0.373207</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.180226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-04 16:45:00</td>\n",
       "      <td>0.086336</td>\n",
       "      <td>0.055423</td>\n",
       "      <td>0.030913</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634516</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.549181</td>\n",
       "      <td>0.494808</td>\n",
       "      <td>0.432932</td>\n",
       "      <td>0.373207</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.180226</td>\n",
       "      <td>0.127954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-04 17:00:00</td>\n",
       "      <td>0.055423</td>\n",
       "      <td>0.030913</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.549181</td>\n",
       "      <td>0.494808</td>\n",
       "      <td>0.432932</td>\n",
       "      <td>0.373207</td>\n",
       "      <td>0.323086</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.180226</td>\n",
       "      <td>0.127954</td>\n",
       "      <td>0.087437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-31 14:15:00</td>\n",
       "      <td>0.745493</td>\n",
       "      <td>0.700954</td>\n",
       "      <td>0.651113</td>\n",
       "      <td>0.579003</td>\n",
       "      <td>0.506893</td>\n",
       "      <td>0.444327</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835631</td>\n",
       "      <td>0.860021</td>\n",
       "      <td>0.871686</td>\n",
       "      <td>0.822906</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.852598</td>\n",
       "      <td>0.825027</td>\n",
       "      <td>0.850477</td>\n",
       "      <td>0.849417</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-31 14:30:00</td>\n",
       "      <td>0.700954</td>\n",
       "      <td>0.651113</td>\n",
       "      <td>0.579003</td>\n",
       "      <td>0.506893</td>\n",
       "      <td>0.444327</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860021</td>\n",
       "      <td>0.871686</td>\n",
       "      <td>0.822906</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.852598</td>\n",
       "      <td>0.825027</td>\n",
       "      <td>0.850477</td>\n",
       "      <td>0.849417</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.802757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-31 14:45:00</td>\n",
       "      <td>0.651113</td>\n",
       "      <td>0.579003</td>\n",
       "      <td>0.506893</td>\n",
       "      <td>0.444327</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.107105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871686</td>\n",
       "      <td>0.822906</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.852598</td>\n",
       "      <td>0.825027</td>\n",
       "      <td>0.850477</td>\n",
       "      <td>0.849417</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.802757</td>\n",
       "      <td>0.777306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-31 15:00:00</td>\n",
       "      <td>0.579003</td>\n",
       "      <td>0.506893</td>\n",
       "      <td>0.444327</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.107105</td>\n",
       "      <td>0.020148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822906</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.852598</td>\n",
       "      <td>0.825027</td>\n",
       "      <td>0.850477</td>\n",
       "      <td>0.849417</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.802757</td>\n",
       "      <td>0.777306</td>\n",
       "      <td>0.730647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-31 15:15:00</td>\n",
       "      <td>0.506893</td>\n",
       "      <td>0.444327</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.107105</td>\n",
       "      <td>0.020148</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.852598</td>\n",
       "      <td>0.825027</td>\n",
       "      <td>0.850477</td>\n",
       "      <td>0.849417</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.802757</td>\n",
       "      <td>0.777306</td>\n",
       "      <td>0.730647</td>\n",
       "      <td>0.659597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19677 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     var1(t-48)  var1(t-47)  var1(t-46)  var1(t-45)  \\\n",
       "index                                                                 \n",
       "2017-01-04 16:00:00    0.323086    0.221793    0.136457    0.086336   \n",
       "2017-01-04 16:15:00    0.221793    0.136457    0.086336    0.055423   \n",
       "2017-01-04 16:30:00    0.136457    0.086336    0.055423    0.030913   \n",
       "2017-01-04 16:45:00    0.086336    0.055423    0.030913    0.010654   \n",
       "2017-01-04 17:00:00    0.055423    0.030913    0.010654    0.000000   \n",
       "...                         ...         ...         ...         ...   \n",
       "2018-12-31 14:15:00    0.745493    0.700954    0.651113    0.579003   \n",
       "2018-12-31 14:30:00    0.700954    0.651113    0.579003    0.506893   \n",
       "2018-12-31 14:45:00    0.651113    0.579003    0.506893    0.444327   \n",
       "2018-12-31 15:00:00    0.579003    0.506893    0.444327    0.363733   \n",
       "2018-12-31 15:15:00    0.506893    0.444327    0.363733    0.268293   \n",
       "\n",
       "                     var1(t-44)  var1(t-43)  var1(t-42)  var1(t-41)  \\\n",
       "index                                                                 \n",
       "2017-01-04 16:00:00    0.055423    0.030913    0.010654    0.000000   \n",
       "2017-01-04 16:15:00    0.030913    0.010654    0.000000    0.000000   \n",
       "2017-01-04 16:30:00    0.010654    0.000000    0.000000    0.000000   \n",
       "2017-01-04 16:45:00    0.000000    0.000000    0.000000    0.000000   \n",
       "2017-01-04 17:00:00    0.000000    0.000000    0.000000    0.000000   \n",
       "...                         ...         ...         ...         ...   \n",
       "2018-12-31 14:15:00    0.506893    0.444327    0.363733    0.268293   \n",
       "2018-12-31 14:30:00    0.444327    0.363733    0.268293    0.195122   \n",
       "2018-12-31 14:45:00    0.363733    0.268293    0.195122    0.147402   \n",
       "2018-12-31 15:00:00    0.268293    0.195122    0.147402    0.142100   \n",
       "2018-12-31 15:15:00    0.195122    0.147402    0.142100    0.107105   \n",
       "\n",
       "                     var1(t-40)  var1(t-39)  ...  var1(t-10)  var1(t-9)  \\\n",
       "index                                        ...                          \n",
       "2017-01-04 16:00:00    0.000000    0.000000  ...    0.693141   0.679285   \n",
       "2017-01-04 16:15:00    0.000000    0.000000  ...    0.679285   0.664379   \n",
       "2017-01-04 16:30:00    0.000000    0.000000  ...    0.664379   0.634516   \n",
       "2017-01-04 16:45:00    0.000000    0.000000  ...    0.634516   0.596100   \n",
       "2017-01-04 17:00:00    0.000000    0.000000  ...    0.596100   0.549181   \n",
       "...                         ...         ...  ...         ...        ...   \n",
       "2018-12-31 14:15:00    0.195122    0.147402  ...    0.835631   0.860021   \n",
       "2018-12-31 14:30:00    0.147402    0.142100  ...    0.860021   0.871686   \n",
       "2018-12-31 14:45:00    0.142100    0.107105  ...    0.871686   0.822906   \n",
       "2018-12-31 15:00:00    0.107105    0.020148  ...    0.822906   0.820785   \n",
       "2018-12-31 15:15:00    0.020148    0.005302  ...    0.820785   0.852598   \n",
       "\n",
       "                     var1(t-8)  var1(t-7)  var1(t-6)  var1(t-5)  var1(t-4)  \\\n",
       "index                                                                        \n",
       "2017-01-04 16:00:00   0.664379   0.634516   0.596100   0.549181   0.494808   \n",
       "2017-01-04 16:15:00   0.634516   0.596100   0.549181   0.494808   0.432932   \n",
       "2017-01-04 16:30:00   0.596100   0.549181   0.494808   0.432932   0.373207   \n",
       "2017-01-04 16:45:00   0.549181   0.494808   0.432932   0.373207   0.323086   \n",
       "2017-01-04 17:00:00   0.494808   0.432932   0.373207   0.323086   0.247404   \n",
       "...                        ...        ...        ...        ...        ...   \n",
       "2018-12-31 14:15:00   0.871686   0.822906   0.820785   0.852598   0.825027   \n",
       "2018-12-31 14:30:00   0.822906   0.820785   0.852598   0.825027   0.850477   \n",
       "2018-12-31 14:45:00   0.820785   0.852598   0.825027   0.850477   0.849417   \n",
       "2018-12-31 15:00:00   0.852598   0.825027   0.850477   0.849417   0.782609   \n",
       "2018-12-31 15:15:00   0.825027   0.850477   0.849417   0.782609   0.802757   \n",
       "\n",
       "                     var1(t-3)  var1(t-2)  var1(t-1)  \n",
       "index                                                 \n",
       "2017-01-04 16:00:00   0.432932   0.373207   0.323086  \n",
       "2017-01-04 16:15:00   0.373207   0.323086   0.247404  \n",
       "2017-01-04 16:30:00   0.323086   0.247404   0.180226  \n",
       "2017-01-04 16:45:00   0.247404   0.180226   0.127954  \n",
       "2017-01-04 17:00:00   0.180226   0.127954   0.087437  \n",
       "...                        ...        ...        ...  \n",
       "2018-12-31 14:15:00   0.850477   0.849417   0.782609  \n",
       "2018-12-31 14:30:00   0.849417   0.782609   0.802757  \n",
       "2018-12-31 14:45:00   0.782609   0.802757   0.777306  \n",
       "2018-12-31 15:00:00   0.802757   0.777306   0.730647  \n",
       "2018-12-31 15:15:00   0.777306   0.730647   0.659597  \n",
       "\n",
       "[19677 rows x 48 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_stable_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16809 samples, validate on 1868 samples\n",
      "Epoch 1/100\n",
      "16809/16809 [==============================] - 1s 53us/step - loss: 0.0886 - val_loss: 0.0307\n",
      "Epoch 2/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0483 - val_loss: 0.0158\n",
      "Epoch 3/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0320 - val_loss: 0.0133\n",
      "Epoch 4/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0252 - val_loss: 0.0115\n",
      "Epoch 5/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0229 - val_loss: 0.0109\n",
      "Epoch 6/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0212 - val_loss: 0.0100\n",
      "Epoch 7/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0199 - val_loss: 0.0096\n",
      "Epoch 8/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0182 - val_loss: 0.0089\n",
      "Epoch 9/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0171 - val_loss: 0.0086\n",
      "Epoch 10/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0161 - val_loss: 0.0080\n",
      "Epoch 11/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0160 - val_loss: 0.0078\n",
      "Epoch 12/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0151 - val_loss: 0.0075\n",
      "Epoch 13/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0146 - val_loss: 0.0077\n",
      "Epoch 14/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0138 - val_loss: 0.0073\n",
      "Epoch 15/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0136 - val_loss: 0.0069\n",
      "Epoch 16/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0131 - val_loss: 0.0068\n",
      "Epoch 17/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0130 - val_loss: 0.0068\n",
      "Epoch 18/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0126 - val_loss: 0.0066\n",
      "Epoch 19/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0125 - val_loss: 0.0065\n",
      "Epoch 20/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0120 - val_loss: 0.0065\n",
      "Epoch 21/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0120 - val_loss: 0.0064\n",
      "Epoch 22/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0116 - val_loss: 0.0065\n",
      "Epoch 23/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0115 - val_loss: 0.0061\n",
      "Epoch 24/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0113 - val_loss: 0.0066\n",
      "Epoch 25/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 26/100\n",
      "16809/16809 [==============================] - 0s 9us/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 27/100\n",
      "16809/16809 [==============================] - 0s 9us/step - loss: 0.0109 - val_loss: 0.0061\n",
      "Epoch 28/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0105 - val_loss: 0.0062\n",
      "Epoch 29/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0106 - val_loss: 0.0059\n",
      "Epoch 30/100\n",
      "16809/16809 [==============================] - 0s 9us/step - loss: 0.0103 - val_loss: 0.0058\n",
      "Epoch 31/100\n",
      "16809/16809 [==============================] - 0s 10us/step - loss: 0.0104 - val_loss: 0.0060\n",
      "Epoch 32/100\n",
      "16809/16809 [==============================] - 0s 9us/step - loss: 0.0102 - val_loss: 0.0057\n",
      "Epoch 33/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0101 - val_loss: 0.0057\n",
      "Epoch 34/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0101 - val_loss: 0.0056\n",
      "Epoch 35/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0100 - val_loss: 0.0056\n",
      "Epoch 36/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0101 - val_loss: 0.0057\n",
      "Epoch 37/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0098 - val_loss: 0.0059\n",
      "Epoch 38/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0096 - val_loss: 0.0056\n",
      "Epoch 39/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0097 - val_loss: 0.0055\n",
      "Epoch 40/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0095 - val_loss: 0.0057\n",
      "Epoch 41/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0094 - val_loss: 0.0053\n",
      "Epoch 42/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0094 - val_loss: 0.0053\n",
      "Epoch 43/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0094 - val_loss: 0.0056\n",
      "Epoch 44/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0092 - val_loss: 0.0053\n",
      "Epoch 45/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0093 - val_loss: 0.0056\n",
      "Epoch 46/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0093 - val_loss: 0.0055\n",
      "Epoch 47/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0092 - val_loss: 0.0054\n",
      "Epoch 48/100\n",
      "16809/16809 [==============================] - 0s 7us/step - loss: 0.0092 - val_loss: 0.0055\n",
      "Epoch 49/100\n",
      "16809/16809 [==============================] - 0s 8us/step - loss: 0.0091 - val_loss: 0.0055\n",
      "Train on 16455 samples, validate on 1829 samples\n",
      "Epoch 1/100\n",
      "16455/16455 [==============================] - 1s 54us/step - loss: 0.1203 - val_loss: 0.0551\n",
      "Epoch 2/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0584 - val_loss: 0.0287\n",
      "Epoch 3/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0437 - val_loss: 0.0238\n",
      "Epoch 4/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0365 - val_loss: 0.0228\n",
      "Epoch 5/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0321 - val_loss: 0.0201\n",
      "Epoch 6/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0295 - val_loss: 0.0177\n",
      "Epoch 7/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0268 - val_loss: 0.0172\n",
      "Epoch 8/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0248 - val_loss: 0.0163\n",
      "Epoch 9/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0232 - val_loss: 0.0155\n",
      "Epoch 10/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0225 - val_loss: 0.0142\n",
      "Epoch 11/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0209 - val_loss: 0.0137\n",
      "Epoch 12/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0199 - val_loss: 0.0137\n",
      "Epoch 13/100\n",
      "16455/16455 [==============================] - 0s 8us/step - loss: 0.0191 - val_loss: 0.0130\n",
      "Epoch 14/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0184 - val_loss: 0.0123\n",
      "Epoch 15/100\n",
      "16455/16455 [==============================] - 0s 8us/step - loss: 0.0179 - val_loss: 0.0124\n",
      "Epoch 16/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0175 - val_loss: 0.0118\n",
      "Epoch 17/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0169 - val_loss: 0.0120\n",
      "Epoch 18/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0167 - val_loss: 0.0119\n",
      "Epoch 19/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0164 - val_loss: 0.0113\n",
      "Epoch 20/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0162 - val_loss: 0.0113\n",
      "Epoch 21/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0158 - val_loss: 0.0113\n",
      "Epoch 22/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0157 - val_loss: 0.0115\n",
      "Epoch 23/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0152 - val_loss: 0.0111\n",
      "Epoch 24/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0153 - val_loss: 0.0108\n",
      "Epoch 25/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0151 - val_loss: 0.0107\n",
      "Epoch 26/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0149 - val_loss: 0.0106\n",
      "Epoch 27/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0148 - val_loss: 0.0105\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0146 - val_loss: 0.0107\n",
      "Epoch 29/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0143 - val_loss: 0.0104\n",
      "Epoch 30/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0144 - val_loss: 0.0110\n",
      "Epoch 31/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0141 - val_loss: 0.0105\n",
      "Epoch 32/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0142 - val_loss: 0.0105\n",
      "Epoch 33/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0138 - val_loss: 0.0106\n",
      "Epoch 34/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0140 - val_loss: 0.0102\n",
      "Epoch 35/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0137 - val_loss: 0.0103\n",
      "Epoch 36/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0135 - val_loss: 0.0105\n",
      "Epoch 37/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0134 - val_loss: 0.0102\n",
      "Epoch 38/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0134 - val_loss: 0.0103\n",
      "Epoch 39/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0136 - val_loss: 0.0102\n",
      "Epoch 40/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0134 - val_loss: 0.0102\n",
      "Epoch 41/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0134 - val_loss: 0.0104\n",
      "Epoch 42/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0132 - val_loss: 0.0100\n",
      "Epoch 43/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0131 - val_loss: 0.0100\n",
      "Epoch 44/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0127 - val_loss: 0.0097\n",
      "Epoch 45/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0130 - val_loss: 0.0098\n",
      "Epoch 46/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0130 - val_loss: 0.0099\n",
      "Epoch 47/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0130 - val_loss: 0.0098\n",
      "Epoch 48/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0128 - val_loss: 0.0102\n",
      "Epoch 49/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0128 - val_loss: 0.0096\n",
      "Epoch 50/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0126 - val_loss: 0.0101\n",
      "Epoch 51/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0127 - val_loss: 0.0098\n",
      "Epoch 52/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0126 - val_loss: 0.0097\n",
      "Epoch 53/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0125 - val_loss: 0.0098\n",
      "Epoch 54/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0126 - val_loss: 0.0096\n",
      "Epoch 55/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0126 - val_loss: 0.0097\n",
      "Epoch 56/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0125 - val_loss: 0.0095\n",
      "Epoch 57/100\n",
      "16455/16455 [==============================] - 0s 8us/step - loss: 0.0124 - val_loss: 0.0097\n",
      "Epoch 58/100\n",
      "16455/16455 [==============================] - 0s 8us/step - loss: 0.0125 - val_loss: 0.0096\n",
      "Epoch 59/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0123 - val_loss: 0.0096\n",
      "Epoch 60/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0125 - val_loss: 0.0096\n",
      "Epoch 61/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0124 - val_loss: 0.0094\n",
      "Epoch 62/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0122 - val_loss: 0.0094\n",
      "Epoch 63/100\n",
      "16455/16455 [==============================] - 0s 8us/step - loss: 0.0124 - val_loss: 0.0097\n",
      "Epoch 64/100\n",
      "16455/16455 [==============================] - 0s 8us/step - loss: 0.0124 - val_loss: 0.0096\n",
      "Epoch 65/100\n",
      "16455/16455 [==============================] - 0s 8us/step - loss: 0.0122 - val_loss: 0.0095\n",
      "Epoch 66/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0123 - val_loss: 0.0093\n",
      "Epoch 67/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0123 - val_loss: 0.0095\n",
      "Epoch 68/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0123 - val_loss: 0.0095\n",
      "Epoch 69/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0123 - val_loss: 0.0092\n",
      "Epoch 70/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0125 - val_loss: 0.0093\n",
      "Epoch 71/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0122 - val_loss: 0.0093\n",
      "Epoch 72/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0123 - val_loss: 0.0094\n",
      "Epoch 73/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0123 - val_loss: 0.0093\n",
      "Epoch 74/100\n",
      "16455/16455 [==============================] - 0s 7us/step - loss: 0.0120 - val_loss: 0.0093\n",
      "Train on 16100 samples, validate on 1789 samples\n",
      "Epoch 1/100\n",
      "16100/16100 [==============================] - 1s 56us/step - loss: 0.1071 - val_loss: 0.0457\n",
      "Epoch 2/100\n",
      "16100/16100 [==============================] - 0s 8us/step - loss: 0.0541 - val_loss: 0.0261\n",
      "Epoch 3/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0377 - val_loss: 0.0226\n",
      "Epoch 4/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0316 - val_loss: 0.0200\n",
      "Epoch 5/100\n",
      "16100/16100 [==============================] - 0s 8us/step - loss: 0.0289 - val_loss: 0.0186\n",
      "Epoch 6/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0266 - val_loss: 0.0178\n",
      "Epoch 7/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0255 - val_loss: 0.0170\n",
      "Epoch 8/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0244 - val_loss: 0.0166\n",
      "Epoch 9/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0230 - val_loss: 0.0165\n",
      "Epoch 10/100\n",
      "16100/16100 [==============================] - 0s 8us/step - loss: 0.0222 - val_loss: 0.0160\n",
      "Epoch 11/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0213 - val_loss: 0.0160\n",
      "Epoch 12/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0208 - val_loss: 0.0160\n",
      "Epoch 13/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0200 - val_loss: 0.0160\n",
      "Epoch 14/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0197 - val_loss: 0.0151\n",
      "Epoch 15/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0193 - val_loss: 0.0152\n",
      "Epoch 16/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0190 - val_loss: 0.0152\n",
      "Epoch 17/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0188 - val_loss: 0.0150\n",
      "Epoch 18/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0185 - val_loss: 0.0146\n",
      "Epoch 19/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0181 - val_loss: 0.0146\n",
      "Epoch 20/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0179 - val_loss: 0.0147\n",
      "Epoch 21/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0176 - val_loss: 0.0143\n",
      "Epoch 22/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0177 - val_loss: 0.0144\n",
      "Epoch 23/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0173 - val_loss: 0.0142\n",
      "Epoch 24/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0172 - val_loss: 0.0144\n",
      "Epoch 25/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0173 - val_loss: 0.0144\n",
      "Epoch 26/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0169 - val_loss: 0.0142\n",
      "Epoch 27/100\n",
      "16100/16100 [==============================] - 0s 8us/step - loss: 0.0168 - val_loss: 0.0142\n",
      "Epoch 28/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0167 - val_loss: 0.0138\n",
      "Epoch 29/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0164 - val_loss: 0.0137\n",
      "Epoch 30/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0164 - val_loss: 0.0140\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16100/16100 [==============================] - 0s 8us/step - loss: 0.0165 - val_loss: 0.0140\n",
      "Epoch 32/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0164 - val_loss: 0.0140\n",
      "Epoch 33/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0161 - val_loss: 0.0136\n",
      "Epoch 34/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0158 - val_loss: 0.0138\n",
      "Epoch 35/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0162 - val_loss: 0.0135\n",
      "Epoch 36/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0161 - val_loss: 0.0135\n",
      "Epoch 37/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0160 - val_loss: 0.0136\n",
      "Epoch 38/100\n",
      "16100/16100 [==============================] - 0s 8us/step - loss: 0.0161 - val_loss: 0.0137\n",
      "Epoch 39/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0158 - val_loss: 0.0139\n",
      "Epoch 40/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0158 - val_loss: 0.0133\n",
      "Epoch 41/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0158 - val_loss: 0.0132\n",
      "Epoch 42/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0159 - val_loss: 0.0132\n",
      "Epoch 43/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0156 - val_loss: 0.0135\n",
      "Epoch 44/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0156 - val_loss: 0.0132\n",
      "Epoch 45/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 46/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0157 - val_loss: 0.0133\n",
      "Epoch 47/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 48/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0154 - val_loss: 0.0137\n",
      "Epoch 49/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0154 - val_loss: 0.0134\n",
      "Epoch 50/100\n",
      "16100/16100 [==============================] - 0s 7us/step - loss: 0.0153 - val_loss: 0.0133\n",
      "Train on 15746 samples, validate on 1750 samples\n",
      "Epoch 1/100\n",
      "15746/15746 [==============================] - 1s 65us/step - loss: 0.0914 - val_loss: 0.0396\n",
      "Epoch 2/100\n",
      "15746/15746 [==============================] - 0s 8us/step - loss: 0.0475 - val_loss: 0.0267\n",
      "Epoch 3/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0350 - val_loss: 0.0231\n",
      "Epoch 4/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0313 - val_loss: 0.0216\n",
      "Epoch 5/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0294 - val_loss: 0.0212\n",
      "Epoch 6/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0278 - val_loss: 0.0209\n",
      "Epoch 7/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0266 - val_loss: 0.0208\n",
      "Epoch 8/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0256 - val_loss: 0.0202\n",
      "Epoch 9/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0249 - val_loss: 0.0196\n",
      "Epoch 10/100\n",
      "15746/15746 [==============================] - 0s 8us/step - loss: 0.0247 - val_loss: 0.0203\n",
      "Epoch 11/100\n",
      "15746/15746 [==============================] - 0s 8us/step - loss: 0.0239 - val_loss: 0.0199\n",
      "Epoch 12/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0234 - val_loss: 0.0194\n",
      "Epoch 13/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0234 - val_loss: 0.0194\n",
      "Epoch 14/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0226 - val_loss: 0.0193\n",
      "Epoch 15/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0222 - val_loss: 0.0190\n",
      "Epoch 16/100\n",
      "15746/15746 [==============================] - 0s 8us/step - loss: 0.0221 - val_loss: 0.0185\n",
      "Epoch 17/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0217 - val_loss: 0.0185\n",
      "Epoch 18/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0215 - val_loss: 0.0182\n",
      "Epoch 19/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0213 - val_loss: 0.0185\n",
      "Epoch 20/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0208 - val_loss: 0.0180\n",
      "Epoch 21/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0208 - val_loss: 0.0181\n",
      "Epoch 22/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0207 - val_loss: 0.0185\n",
      "Epoch 23/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0204 - val_loss: 0.0177\n",
      "Epoch 24/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0199 - val_loss: 0.0182\n",
      "Epoch 25/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0200 - val_loss: 0.0180\n",
      "Epoch 26/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0198 - val_loss: 0.0171\n",
      "Epoch 27/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0196 - val_loss: 0.0170\n",
      "Epoch 28/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0197 - val_loss: 0.0170\n",
      "Epoch 29/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0193 - val_loss: 0.0171\n",
      "Epoch 30/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0192 - val_loss: 0.0173\n",
      "Epoch 31/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0192 - val_loss: 0.0168\n",
      "Epoch 32/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0189 - val_loss: 0.0175\n",
      "Epoch 33/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0190 - val_loss: 0.0168\n",
      "Epoch 34/100\n",
      "15746/15746 [==============================] - 0s 8us/step - loss: 0.0188 - val_loss: 0.0168\n",
      "Epoch 35/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0186 - val_loss: 0.0172\n",
      "Epoch 36/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0184 - val_loss: 0.0166\n",
      "Epoch 37/100\n",
      "15746/15746 [==============================] - 0s 8us/step - loss: 0.0184 - val_loss: 0.0166\n",
      "Epoch 38/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0184 - val_loss: 0.0169\n",
      "Epoch 39/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0186 - val_loss: 0.0168\n",
      "Epoch 40/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0181 - val_loss: 0.0167\n",
      "Epoch 41/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0182 - val_loss: 0.0165\n",
      "Epoch 42/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0183 - val_loss: 0.0171\n",
      "Epoch 43/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0181 - val_loss: 0.0167\n",
      "Epoch 44/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0181 - val_loss: 0.0166\n",
      "Epoch 45/100\n",
      "15746/15746 [==============================] - 0s 8us/step - loss: 0.0179 - val_loss: 0.0166\n",
      "Epoch 46/100\n",
      "15746/15746 [==============================] - 0s 7us/step - loss: 0.0180 - val_loss: 0.0169\n",
      "Train on 15393 samples, validate on 1711 samples\n",
      "Epoch 1/100\n",
      "15393/15393 [==============================] - 1s 63us/step - loss: 0.1577 - val_loss: 0.0883\n",
      "Epoch 2/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0862 - val_loss: 0.0483\n",
      "Epoch 3/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0674 - val_loss: 0.0420\n",
      "Epoch 4/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0546 - val_loss: 0.0379\n",
      "Epoch 5/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0478 - val_loss: 0.0338\n",
      "Epoch 6/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0434 - val_loss: 0.0321\n",
      "Epoch 7/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0404 - val_loss: 0.0300\n",
      "Epoch 8/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0377 - val_loss: 0.0284\n",
      "Epoch 9/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0356 - val_loss: 0.0279\n",
      "Epoch 10/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0351 - val_loss: 0.0266\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0334 - val_loss: 0.0257\n",
      "Epoch 12/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0317 - val_loss: 0.0249\n",
      "Epoch 13/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0305 - val_loss: 0.0241\n",
      "Epoch 14/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0295 - val_loss: 0.0238\n",
      "Epoch 15/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0292 - val_loss: 0.0235\n",
      "Epoch 16/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0283 - val_loss: 0.0231\n",
      "Epoch 17/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0277 - val_loss: 0.0229\n",
      "Epoch 18/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0275 - val_loss: 0.0227\n",
      "Epoch 19/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0272 - val_loss: 0.0227\n",
      "Epoch 20/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0268 - val_loss: 0.0223\n",
      "Epoch 21/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0268 - val_loss: 0.0223\n",
      "Epoch 22/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0261 - val_loss: 0.0222\n",
      "Epoch 23/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0261 - val_loss: 0.0222\n",
      "Epoch 24/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0259 - val_loss: 0.0220\n",
      "Epoch 25/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0258 - val_loss: 0.0222\n",
      "Epoch 26/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0254 - val_loss: 0.0221\n",
      "Epoch 27/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0252 - val_loss: 0.0219\n",
      "Epoch 28/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0251 - val_loss: 0.0217\n",
      "Epoch 29/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0250 - val_loss: 0.0216\n",
      "Epoch 30/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0253 - val_loss: 0.0216\n",
      "Epoch 31/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0246 - val_loss: 0.0220\n",
      "Epoch 32/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0244 - val_loss: 0.0219\n",
      "Epoch 33/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0244 - val_loss: 0.0222\n",
      "Epoch 34/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0240 - val_loss: 0.0213\n",
      "Epoch 35/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0247 - val_loss: 0.0212\n",
      "Epoch 36/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0240 - val_loss: 0.0214\n",
      "Epoch 37/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0238 - val_loss: 0.0213\n",
      "Epoch 38/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0237 - val_loss: 0.0213\n",
      "Epoch 39/100\n",
      "15393/15393 [==============================] - 0s 7us/step - loss: 0.0237 - val_loss: 0.0212\n",
      "Epoch 40/100\n",
      "15393/15393 [==============================] - 0s 8us/step - loss: 0.0238 - val_loss: 0.0212\n",
      "Train on 15039 samples, validate on 1672 samples\n",
      "Epoch 1/100\n",
      "15039/15039 [==============================] - 1s 63us/step - loss: 0.1393 - val_loss: 0.0888\n",
      "Epoch 2/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0914 - val_loss: 0.0492\n",
      "Epoch 3/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0569 - val_loss: 0.0300\n",
      "Epoch 4/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0419 - val_loss: 0.0269\n",
      "Epoch 5/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0361 - val_loss: 0.0275\n",
      "Epoch 6/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0340 - val_loss: 0.0253\n",
      "Epoch 7/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0326 - val_loss: 0.0249\n",
      "Epoch 8/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0315 - val_loss: 0.0251\n",
      "Epoch 9/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0305 - val_loss: 0.0246\n",
      "Epoch 10/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0300 - val_loss: 0.0241\n",
      "Epoch 11/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0300 - val_loss: 0.0236\n",
      "Epoch 12/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0292 - val_loss: 0.0234\n",
      "Epoch 13/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0289 - val_loss: 0.0234\n",
      "Epoch 14/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0291 - val_loss: 0.0234\n",
      "Epoch 15/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0283 - val_loss: 0.0229\n",
      "Epoch 16/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0278 - val_loss: 0.0228\n",
      "Epoch 17/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0275 - val_loss: 0.0228\n",
      "Epoch 18/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0269 - val_loss: 0.0226\n",
      "Epoch 19/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0271 - val_loss: 0.0224\n",
      "Epoch 20/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0269 - val_loss: 0.0230\n",
      "Epoch 21/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0262 - val_loss: 0.0225\n",
      "Epoch 22/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0265 - val_loss: 0.0222\n",
      "Epoch 23/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0260 - val_loss: 0.0219\n",
      "Epoch 24/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0263 - val_loss: 0.0221\n",
      "Epoch 25/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0256 - val_loss: 0.0220\n",
      "Epoch 26/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0260 - val_loss: 0.0219\n",
      "Epoch 27/100\n",
      "15039/15039 [==============================] - 0s 8us/step - loss: 0.0259 - val_loss: 0.0221\n",
      "Epoch 28/100\n",
      "15039/15039 [==============================] - 0s 7us/step - loss: 0.0253 - val_loss: 0.0221\n",
      "Train on 14683 samples, validate on 1632 samples\n",
      "Epoch 1/100\n",
      "14683/14683 [==============================] - 1s 65us/step - loss: 0.0831 - val_loss: 0.0365\n",
      "Epoch 2/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0568 - val_loss: 0.0329\n",
      "Epoch 3/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0468 - val_loss: 0.0315\n",
      "Epoch 4/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0407 - val_loss: 0.0283\n",
      "Epoch 5/100\n",
      "14683/14683 [==============================] - 0s 7us/step - loss: 0.0365 - val_loss: 0.0285\n",
      "Epoch 6/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0354 - val_loss: 0.0259\n",
      "Epoch 7/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0334 - val_loss: 0.0255\n",
      "Epoch 8/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0326 - val_loss: 0.0252\n",
      "Epoch 9/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0323 - val_loss: 0.0242\n",
      "Epoch 10/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0312 - val_loss: 0.0237\n",
      "Epoch 11/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0311 - val_loss: 0.0234\n",
      "Epoch 12/100\n",
      "14683/14683 [==============================] - 0s 7us/step - loss: 0.0302 - val_loss: 0.0253\n",
      "Epoch 13/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0300 - val_loss: 0.0234\n",
      "Epoch 14/100\n",
      "14683/14683 [==============================] - 0s 7us/step - loss: 0.0296 - val_loss: 0.0235\n",
      "Epoch 15/100\n",
      "14683/14683 [==============================] - 0s 7us/step - loss: 0.0291 - val_loss: 0.0235\n",
      "Epoch 16/100\n",
      "14683/14683 [==============================] - 0s 8us/step - loss: 0.0286 - val_loss: 0.0235\n",
      "Train on 14328 samples, validate on 1592 samples\n",
      "Epoch 1/100\n",
      "14328/14328 [==============================] - 1s 66us/step - loss: 0.1298 - val_loss: 0.0605\n",
      "Epoch 2/100\n",
      "14328/14328 [==============================] - 0s 8us/step - loss: 0.0666 - val_loss: 0.0457\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14328/14328 [==============================] - 0s 8us/step - loss: 0.0501 - val_loss: 0.0372\n",
      "Epoch 4/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0429 - val_loss: 0.0329\n",
      "Epoch 5/100\n",
      "14328/14328 [==============================] - 0s 8us/step - loss: 0.0386 - val_loss: 0.0298\n",
      "Epoch 6/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0365 - val_loss: 0.0279\n",
      "Epoch 7/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0352 - val_loss: 0.0267\n",
      "Epoch 8/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0344 - val_loss: 0.0264\n",
      "Epoch 9/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0335 - val_loss: 0.0259\n",
      "Epoch 10/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0334 - val_loss: 0.0257\n",
      "Epoch 11/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0327 - val_loss: 0.0256\n",
      "Epoch 12/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0322 - val_loss: 0.0255\n",
      "Epoch 13/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0327 - val_loss: 0.0258\n",
      "Epoch 14/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0320 - val_loss: 0.0249\n",
      "Epoch 15/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0315 - val_loss: 0.0249\n",
      "Epoch 16/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0308 - val_loss: 0.0242\n",
      "Epoch 17/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0309 - val_loss: 0.0242\n",
      "Epoch 18/100\n",
      "14328/14328 [==============================] - 0s 8us/step - loss: 0.0302 - val_loss: 0.0241\n",
      "Epoch 19/100\n",
      "14328/14328 [==============================] - 0s 9us/step - loss: 0.0303 - val_loss: 0.0245\n",
      "Epoch 20/100\n",
      "14328/14328 [==============================] - 0s 8us/step - loss: 0.0300 - val_loss: 0.0241\n",
      "Epoch 21/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0295 - val_loss: 0.0246\n",
      "Epoch 22/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0297 - val_loss: 0.0237\n",
      "Epoch 23/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0295 - val_loss: 0.0241\n",
      "Epoch 24/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0297 - val_loss: 0.0242\n",
      "Epoch 25/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0294 - val_loss: 0.0242\n",
      "Epoch 26/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0295 - val_loss: 0.0244\n",
      "Epoch 27/100\n",
      "14328/14328 [==============================] - 0s 7us/step - loss: 0.0292 - val_loss: 0.0247\n",
      "Train on 13973 samples, validate on 1553 samples\n",
      "Epoch 1/100\n",
      "13973/13973 [==============================] - 1s 69us/step - loss: 0.1177 - val_loss: 0.0454\n",
      "Epoch 2/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0652 - val_loss: 0.0374\n",
      "Epoch 3/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0518 - val_loss: 0.0374\n",
      "Epoch 4/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0457 - val_loss: 0.0345\n",
      "Epoch 5/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0426 - val_loss: 0.0305\n",
      "Epoch 6/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0392 - val_loss: 0.0292\n",
      "Epoch 7/100\n",
      "13973/13973 [==============================] - 0s 7us/step - loss: 0.0376 - val_loss: 0.0269\n",
      "Epoch 8/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0362 - val_loss: 0.0261\n",
      "Epoch 9/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0358 - val_loss: 0.0253\n",
      "Epoch 10/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0346 - val_loss: 0.0249\n",
      "Epoch 11/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0344 - val_loss: 0.0243\n",
      "Epoch 12/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0340 - val_loss: 0.0240\n",
      "Epoch 13/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0328 - val_loss: 0.0240\n",
      "Epoch 14/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0336 - val_loss: 0.0236\n",
      "Epoch 15/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0326 - val_loss: 0.0233\n",
      "Epoch 16/100\n",
      "13973/13973 [==============================] - 0s 7us/step - loss: 0.0327 - val_loss: 0.0235\n",
      "Epoch 17/100\n",
      "13973/13973 [==============================] - 0s 7us/step - loss: 0.0325 - val_loss: 0.0232\n",
      "Epoch 18/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0322 - val_loss: 0.0229\n",
      "Epoch 19/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0318 - val_loss: 0.0228\n",
      "Epoch 20/100\n",
      "13973/13973 [==============================] - 0s 9us/step - loss: 0.0316 - val_loss: 0.0231\n",
      "Epoch 21/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0313 - val_loss: 0.0229\n",
      "Epoch 22/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0310 - val_loss: 0.0226\n",
      "Epoch 23/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0309 - val_loss: 0.0225\n",
      "Epoch 24/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0307 - val_loss: 0.0221\n",
      "Epoch 25/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0305 - val_loss: 0.0226\n",
      "Epoch 26/100\n",
      "13973/13973 [==============================] - 0s 7us/step - loss: 0.0301 - val_loss: 0.0223\n",
      "Epoch 27/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0301 - val_loss: 0.0222\n",
      "Epoch 28/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0302 - val_loss: 0.0227\n",
      "Epoch 29/100\n",
      "13973/13973 [==============================] - 0s 8us/step - loss: 0.0300 - val_loss: 0.0222\n",
      "Train on 13619 samples, validate on 1514 samples\n",
      "Epoch 1/100\n",
      "13619/13619 [==============================] - 1s 73us/step - loss: 0.0812 - val_loss: 0.0451\n",
      "Epoch 2/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0565 - val_loss: 0.0347\n",
      "Epoch 3/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0475 - val_loss: 0.0315\n",
      "Epoch 4/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0419 - val_loss: 0.0291\n",
      "Epoch 5/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0386 - val_loss: 0.0284\n",
      "Epoch 6/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0371 - val_loss: 0.0259\n",
      "Epoch 7/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0358 - val_loss: 0.0255\n",
      "Epoch 8/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0344 - val_loss: 0.0249\n",
      "Epoch 9/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0341 - val_loss: 0.0250\n",
      "Epoch 10/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0333 - val_loss: 0.0240\n",
      "Epoch 11/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0334 - val_loss: 0.0246\n",
      "Epoch 12/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0329 - val_loss: 0.0237\n",
      "Epoch 13/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0325 - val_loss: 0.0233\n",
      "Epoch 14/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0322 - val_loss: 0.0235\n",
      "Epoch 15/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0316 - val_loss: 0.0227\n",
      "Epoch 16/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0316 - val_loss: 0.0226\n",
      "Epoch 17/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0309 - val_loss: 0.0225\n",
      "Epoch 18/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0310 - val_loss: 0.0223\n",
      "Epoch 19/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0309 - val_loss: 0.0228\n",
      "Epoch 20/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0302 - val_loss: 0.0223\n",
      "Epoch 21/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0307 - val_loss: 0.0221\n",
      "Epoch 22/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0300 - val_loss: 0.0222\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0301 - val_loss: 0.0220\n",
      "Epoch 24/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0299 - val_loss: 0.0217\n",
      "Epoch 25/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0299 - val_loss: 0.0216\n",
      "Epoch 26/100\n",
      "13619/13619 [==============================] - 0s 7us/step - loss: 0.0297 - val_loss: 0.0218\n",
      "Epoch 27/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0295 - val_loss: 0.0229\n",
      "Epoch 28/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0294 - val_loss: 0.0218\n",
      "Epoch 29/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0292 - val_loss: 0.0219\n",
      "Epoch 30/100\n",
      "13619/13619 [==============================] - 0s 8us/step - loss: 0.0296 - val_loss: 0.0219\n",
      "Train on 13264 samples, validate on 1474 samples\n",
      "Epoch 1/100\n",
      "13264/13264 [==============================] - 1s 78us/step - loss: 0.1635 - val_loss: 0.0682\n",
      "Epoch 2/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0968 - val_loss: 0.0470\n",
      "Epoch 3/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0599 - val_loss: 0.0327\n",
      "Epoch 4/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0478 - val_loss: 0.0319\n",
      "Epoch 5/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0428 - val_loss: 0.0308\n",
      "Epoch 6/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0401 - val_loss: 0.0291\n",
      "Epoch 7/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0383 - val_loss: 0.0279\n",
      "Epoch 8/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0366 - val_loss: 0.0264\n",
      "Epoch 9/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0354 - val_loss: 0.0261\n",
      "Epoch 10/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0344 - val_loss: 0.0259\n",
      "Epoch 11/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0337 - val_loss: 0.0246\n",
      "Epoch 12/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0331 - val_loss: 0.0243\n",
      "Epoch 13/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0331 - val_loss: 0.0241\n",
      "Epoch 14/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0324 - val_loss: 0.0243\n",
      "Epoch 15/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0321 - val_loss: 0.0236\n",
      "Epoch 16/100\n",
      "13264/13264 [==============================] - ETA: 0s - loss: 0.031 - 0s 8us/step - loss: 0.0317 - val_loss: 0.0231\n",
      "Epoch 17/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0314 - val_loss: 0.0232\n",
      "Epoch 18/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0313 - val_loss: 0.0233\n",
      "Epoch 19/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0309 - val_loss: 0.0233\n",
      "Epoch 20/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0305 - val_loss: 0.0225\n",
      "Epoch 21/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0308 - val_loss: 0.0230\n",
      "Epoch 22/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0309 - val_loss: 0.0231\n",
      "Epoch 23/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0301 - val_loss: 0.0228\n",
      "Epoch 24/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0304 - val_loss: 0.0221\n",
      "Epoch 25/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0301 - val_loss: 0.0230\n",
      "Epoch 26/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0301 - val_loss: 0.0221\n",
      "Epoch 27/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0302 - val_loss: 0.0220\n",
      "Epoch 28/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0294 - val_loss: 0.0223\n",
      "Epoch 29/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0292 - val_loss: 0.0221\n",
      "Epoch 30/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0288 - val_loss: 0.0224\n",
      "Epoch 31/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0291 - val_loss: 0.0227\n",
      "Epoch 32/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0293 - val_loss: 0.0215\n",
      "Epoch 33/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0292 - val_loss: 0.0215\n",
      "Epoch 34/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0293 - val_loss: 0.0215\n",
      "Epoch 35/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0291 - val_loss: 0.0214\n",
      "Epoch 36/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0290 - val_loss: 0.0218\n",
      "Epoch 37/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0289 - val_loss: 0.0221\n",
      "Epoch 38/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0287 - val_loss: 0.0219\n",
      "Epoch 39/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0286 - val_loss: 0.0216\n",
      "Epoch 40/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0287 - val_loss: 0.0213\n",
      "Epoch 41/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0286 - val_loss: 0.0221\n",
      "Epoch 42/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0284 - val_loss: 0.0209\n",
      "Epoch 43/100\n",
      "13264/13264 [==============================] - 0s 7us/step - loss: 0.0284 - val_loss: 0.0220\n",
      "Epoch 44/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0282 - val_loss: 0.0210\n",
      "Epoch 45/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0282 - val_loss: 0.0216\n",
      "Epoch 46/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0283 - val_loss: 0.0217\n",
      "Epoch 47/100\n",
      "13264/13264 [==============================] - 0s 8us/step - loss: 0.0281 - val_loss: 0.0211\n",
      "Train on 12905 samples, validate on 1434 samples\n",
      "Epoch 1/100\n",
      "12905/12905 [==============================] - 1s 78us/step - loss: 0.1346 - val_loss: 0.0841\n",
      "Epoch 2/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0786 - val_loss: 0.0454\n",
      "Epoch 3/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0550 - val_loss: 0.0299\n",
      "Epoch 4/100\n",
      "12905/12905 [==============================] - 0s 7us/step - loss: 0.0459 - val_loss: 0.0269\n",
      "Epoch 5/100\n",
      "12905/12905 [==============================] - 0s 7us/step - loss: 0.0417 - val_loss: 0.0281\n",
      "Epoch 6/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0400 - val_loss: 0.0270\n",
      "Epoch 7/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0382 - val_loss: 0.0263\n",
      "Epoch 8/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0369 - val_loss: 0.0252\n",
      "Epoch 9/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0351 - val_loss: 0.0234\n",
      "Epoch 10/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0343 - val_loss: 0.0254\n",
      "Epoch 11/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0330 - val_loss: 0.0237\n",
      "Epoch 12/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0322 - val_loss: 0.0236\n",
      "Epoch 13/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0319 - val_loss: 0.0233\n",
      "Epoch 14/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0313 - val_loss: 0.0230\n",
      "Epoch 15/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0310 - val_loss: 0.0231\n",
      "Epoch 16/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0304 - val_loss: 0.0219\n",
      "Epoch 17/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0303 - val_loss: 0.0219\n",
      "Epoch 18/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0302 - val_loss: 0.0217\n",
      "Epoch 19/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0302 - val_loss: 0.0231\n",
      "Epoch 20/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0295 - val_loss: 0.0220\n",
      "Epoch 21/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0295 - val_loss: 0.0218\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0296 - val_loss: 0.0222\n",
      "Epoch 23/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0292 - val_loss: 0.0211\n",
      "Epoch 24/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0292 - val_loss: 0.0212\n",
      "Epoch 25/100\n",
      "12905/12905 [==============================] - 0s 7us/step - loss: 0.0287 - val_loss: 0.0219\n",
      "Epoch 26/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0290 - val_loss: 0.0211\n",
      "Epoch 27/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0284 - val_loss: 0.0210\n",
      "Epoch 28/100\n",
      "12905/12905 [==============================] - 0s 9us/step - loss: 0.0288 - val_loss: 0.0206\n",
      "Epoch 29/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0285 - val_loss: 0.0212\n",
      "Epoch 30/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0281 - val_loss: 0.0209\n",
      "Epoch 31/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0285 - val_loss: 0.0216\n",
      "Epoch 32/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0279 - val_loss: 0.0214\n",
      "Epoch 33/100\n",
      "12905/12905 [==============================] - 0s 8us/step - loss: 0.0284 - val_loss: 0.0211\n",
      "Train on 12546 samples, validate on 1394 samples\n",
      "Epoch 1/100\n",
      "12546/12546 [==============================] - 1s 87us/step - loss: 0.0816 - val_loss: 0.0399\n",
      "Epoch 2/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0513 - val_loss: 0.0279\n",
      "Epoch 3/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0421 - val_loss: 0.0243\n",
      "Epoch 4/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0364 - val_loss: 0.0237\n",
      "Epoch 5/100\n",
      "12546/12546 [==============================] - 0s 7us/step - loss: 0.0350 - val_loss: 0.0229\n",
      "Epoch 6/100\n",
      "12546/12546 [==============================] - 0s 7us/step - loss: 0.0333 - val_loss: 0.0223\n",
      "Epoch 7/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0329 - val_loss: 0.0226\n",
      "Epoch 8/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0317 - val_loss: 0.0225\n",
      "Epoch 9/100\n",
      "12546/12546 [==============================] - 0s 7us/step - loss: 0.0317 - val_loss: 0.0214\n",
      "Epoch 10/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0308 - val_loss: 0.0207\n",
      "Epoch 11/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0306 - val_loss: 0.0205\n",
      "Epoch 12/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0298 - val_loss: 0.0203\n",
      "Epoch 13/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0298 - val_loss: 0.0202\n",
      "Epoch 14/100\n",
      "12546/12546 [==============================] - 0s 7us/step - loss: 0.0293 - val_loss: 0.0199\n",
      "Epoch 15/100\n",
      "12546/12546 [==============================] - 0s 7us/step - loss: 0.0293 - val_loss: 0.0199\n",
      "Epoch 16/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0293 - val_loss: 0.0202\n",
      "Epoch 17/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0288 - val_loss: 0.0191\n",
      "Epoch 18/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0286 - val_loss: 0.0195\n",
      "Epoch 19/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0286 - val_loss: 0.0194\n",
      "Epoch 20/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0284 - val_loss: 0.0193\n",
      "Epoch 21/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0281 - val_loss: 0.0196\n",
      "Epoch 22/100\n",
      "12546/12546 [==============================] - 0s 8us/step - loss: 0.0278 - val_loss: 0.0196\n",
      "Train on 12193 samples, validate on 1355 samples\n",
      "Epoch 1/100\n",
      "12193/12193 [==============================] - 1s 86us/step - loss: 0.1173 - val_loss: 0.0765\n",
      "Epoch 2/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0676 - val_loss: 0.0500\n",
      "Epoch 3/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0526 - val_loss: 0.0371\n",
      "Epoch 4/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0454 - val_loss: 0.0334\n",
      "Epoch 5/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0423 - val_loss: 0.0306\n",
      "Epoch 6/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0375 - val_loss: 0.0297\n",
      "Epoch 7/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0352 - val_loss: 0.0270\n",
      "Epoch 8/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0330 - val_loss: 0.0287\n",
      "Epoch 9/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0321 - val_loss: 0.0257\n",
      "Epoch 10/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0309 - val_loss: 0.0256\n",
      "Epoch 11/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0304 - val_loss: 0.0242\n",
      "Epoch 12/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0295 - val_loss: 0.0232\n",
      "Epoch 13/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0289 - val_loss: 0.0228\n",
      "Epoch 14/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0283 - val_loss: 0.0228\n",
      "Epoch 15/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0282 - val_loss: 0.0219\n",
      "Epoch 16/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0280 - val_loss: 0.0217\n",
      "Epoch 17/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0277 - val_loss: 0.0213\n",
      "Epoch 18/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0274 - val_loss: 0.0212\n",
      "Epoch 19/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0267 - val_loss: 0.0216\n",
      "Epoch 20/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0269 - val_loss: 0.0216\n",
      "Epoch 21/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0268 - val_loss: 0.0207\n",
      "Epoch 22/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0269 - val_loss: 0.0208\n",
      "Epoch 23/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0264 - val_loss: 0.0206\n",
      "Epoch 24/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0264 - val_loss: 0.0203\n",
      "Epoch 25/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0263 - val_loss: 0.0201\n",
      "Epoch 26/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0258 - val_loss: 0.0207\n",
      "Epoch 27/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0263 - val_loss: 0.0201\n",
      "Epoch 28/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0259 - val_loss: 0.0197\n",
      "Epoch 29/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0256 - val_loss: 0.0196\n",
      "Epoch 30/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0255 - val_loss: 0.0196\n",
      "Epoch 31/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0254 - val_loss: 0.0192\n",
      "Epoch 32/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0260 - val_loss: 0.0192\n",
      "Epoch 33/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0253 - val_loss: 0.0189\n",
      "Epoch 34/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0256 - val_loss: 0.0192\n",
      "Epoch 35/100\n",
      "12193/12193 [==============================] - 0s 7us/step - loss: 0.0253 - val_loss: 0.0194\n",
      "Epoch 36/100\n",
      "12193/12193 [==============================] - 0s 8us/step - loss: 0.0255 - val_loss: 0.0189\n",
      "Epoch 37/100\n",
      "12193/12193 [==============================] - 0s 7us/step - loss: 0.0252 - val_loss: 0.0193\n",
      "Epoch 38/100\n",
      "12193/12193 [==============================] - 0s 7us/step - loss: 0.0254 - val_loss: 0.0194\n",
      "Train on 11838 samples, validate on 1316 samples\n",
      "Epoch 1/100\n",
      "11838/11838 [==============================] - 1s 90us/step - loss: 0.0778 - val_loss: 0.0426\n",
      "Epoch 2/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0457 - val_loss: 0.0269\n",
      "Epoch 3/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0361 - val_loss: 0.0278\n",
      "Epoch 4/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0338 - val_loss: 0.0276\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0317 - val_loss: 0.0273\n",
      "Epoch 6/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0312 - val_loss: 0.0259\n",
      "Epoch 7/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0302 - val_loss: 0.0248\n",
      "Epoch 8/100\n",
      "11838/11838 [==============================] - 0s 9us/step - loss: 0.0295 - val_loss: 0.0237\n",
      "Epoch 9/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0292 - val_loss: 0.0237\n",
      "Epoch 10/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0287 - val_loss: 0.0238\n",
      "Epoch 11/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0284 - val_loss: 0.0231\n",
      "Epoch 12/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0280 - val_loss: 0.0230\n",
      "Epoch 13/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0275 - val_loss: 0.0228\n",
      "Epoch 14/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0270 - val_loss: 0.0228\n",
      "Epoch 15/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0268 - val_loss: 0.0231\n",
      "Epoch 16/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0267 - val_loss: 0.0225\n",
      "Epoch 17/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0273 - val_loss: 0.0226\n",
      "Epoch 18/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0265 - val_loss: 0.0222\n",
      "Epoch 19/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0266 - val_loss: 0.0220\n",
      "Epoch 20/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0263 - val_loss: 0.0218\n",
      "Epoch 21/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0261 - val_loss: 0.0220\n",
      "Epoch 22/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0262 - val_loss: 0.0220\n",
      "Epoch 23/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0260 - val_loss: 0.0223\n",
      "Epoch 24/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0259 - val_loss: 0.0215\n",
      "Epoch 25/100\n",
      "11838/11838 [==============================] - 0s 8us/step - loss: 0.0254 - val_loss: 0.0215\n",
      "Epoch 26/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0254 - val_loss: 0.0222\n",
      "Epoch 27/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0258 - val_loss: 0.0213\n",
      "Epoch 28/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0255 - val_loss: 0.0213\n",
      "Epoch 29/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0256 - val_loss: 0.0212\n",
      "Epoch 30/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0252 - val_loss: 0.0211\n",
      "Epoch 31/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0251 - val_loss: 0.0207\n",
      "Epoch 32/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0250 - val_loss: 0.0206\n",
      "Epoch 33/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0249 - val_loss: 0.0207\n",
      "Epoch 34/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0250 - val_loss: 0.0206\n",
      "Epoch 35/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0248 - val_loss: 0.0208\n",
      "Epoch 36/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0246 - val_loss: 0.0210\n",
      "Epoch 37/100\n",
      "11838/11838 [==============================] - 0s 7us/step - loss: 0.0248 - val_loss: 0.0210\n",
      "Train on 11484 samples, validate on 1277 samples\n",
      "Epoch 1/100\n",
      "11484/11484 [==============================] - 1s 92us/step - loss: 0.0693 - val_loss: 0.0355\n",
      "Epoch 2/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0403 - val_loss: 0.0302\n",
      "Epoch 3/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0360 - val_loss: 0.0284\n",
      "Epoch 4/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0327 - val_loss: 0.0278\n",
      "Epoch 5/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0315 - val_loss: 0.0267\n",
      "Epoch 6/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0311 - val_loss: 0.0260\n",
      "Epoch 7/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0302 - val_loss: 0.0251\n",
      "Epoch 8/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0293 - val_loss: 0.0249\n",
      "Epoch 9/100\n",
      "11484/11484 [==============================] - 0s 9us/step - loss: 0.0293 - val_loss: 0.0252\n",
      "Epoch 10/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0283 - val_loss: 0.0245\n",
      "Epoch 11/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0279 - val_loss: 0.0247\n",
      "Epoch 12/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0278 - val_loss: 0.0240\n",
      "Epoch 13/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0274 - val_loss: 0.0244\n",
      "Epoch 14/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0275 - val_loss: 0.0242\n",
      "Epoch 15/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0266 - val_loss: 0.0239\n",
      "Epoch 16/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0262 - val_loss: 0.0234\n",
      "Epoch 17/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0264 - val_loss: 0.0238\n",
      "Epoch 18/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0261 - val_loss: 0.0230\n",
      "Epoch 19/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0260 - val_loss: 0.0231\n",
      "Epoch 20/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0255 - val_loss: 0.0227\n",
      "Epoch 21/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0256 - val_loss: 0.0224\n",
      "Epoch 22/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0254 - val_loss: 0.0222\n",
      "Epoch 23/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0253 - val_loss: 0.0224\n",
      "Epoch 24/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0248 - val_loss: 0.0229\n",
      "Epoch 25/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0249 - val_loss: 0.0227\n",
      "Epoch 26/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0251 - val_loss: 0.0226\n",
      "Epoch 27/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0248 - val_loss: 0.0220\n",
      "Epoch 28/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0249 - val_loss: 0.0222\n",
      "Epoch 29/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0247 - val_loss: 0.0222\n",
      "Epoch 30/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0244 - val_loss: 0.0217\n",
      "Epoch 31/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0243 - val_loss: 0.0215\n",
      "Epoch 32/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0240 - val_loss: 0.0219\n",
      "Epoch 33/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0242 - val_loss: 0.0214\n",
      "Epoch 34/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0239 - val_loss: 0.0214\n",
      "Epoch 35/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0239 - val_loss: 0.0214\n",
      "Epoch 36/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0241 - val_loss: 0.0221\n",
      "Epoch 37/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0239 - val_loss: 0.0217\n",
      "Epoch 38/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0239 - val_loss: 0.0213\n",
      "Epoch 39/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0235 - val_loss: 0.0215\n",
      "Epoch 40/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0236 - val_loss: 0.0212\n",
      "Epoch 41/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0233 - val_loss: 0.0211\n",
      "Epoch 42/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0235 - val_loss: 0.0208\n",
      "Epoch 43/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0232 - val_loss: 0.0211\n",
      "Epoch 44/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0232 - val_loss: 0.0212\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0232 - val_loss: 0.0208\n",
      "Epoch 46/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0235 - val_loss: 0.0209\n",
      "Epoch 47/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0232 - val_loss: 0.0210\n",
      "Epoch 48/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0232 - val_loss: 0.0205\n",
      "Epoch 49/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0230 - val_loss: 0.0206\n",
      "Epoch 50/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0228 - val_loss: 0.0207\n",
      "Epoch 51/100\n",
      "11484/11484 [==============================] - 0s 7us/step - loss: 0.0229 - val_loss: 0.0208\n",
      "Epoch 52/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0227 - val_loss: 0.0209\n",
      "Epoch 53/100\n",
      "11484/11484 [==============================] - 0s 8us/step - loss: 0.0233 - val_loss: 0.0205\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# parameters\n",
    "input_len = 48\n",
    "output_step = 1\n",
    "num_feature = 1\n",
    "batch_size = 512\n",
    "epochs = 100\n",
    "\n",
    "test_len = 2880\n",
    "\n",
    "for i in range(16):\n",
    "    \n",
    "    # split train and test(test one month)\n",
    "    train_x,train_y = names['input_stable_%s' % str(i+1)][:-1000],names['output_stable_%s' % str(i+1)][:-1000]\n",
    "    test_x,test_y = names['input_stable_%s' % str(i+1)][-1000:],names['output_stable_%s' % str(i+1)][-1000:]\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(int(input_len/2),input_dim=input_len , activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(output_step, activation='relu'))\n",
    "\n",
    "    adam = Adam(lr=0.001)\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    history = model.fit(train_x,train_y,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_split = 0.1,\n",
    "                        callbacks=[\n",
    "                            TensorBoard(log_dir='/tmp/tensorboard', write_graph=True),\n",
    "                            EarlyStopping(monitor='val_loss', patience=5, mode='auto')\n",
    "                        ]\n",
    "                        )\n",
    "\n",
    "    # make a prediction\n",
    "    names['y_hat_%s' % str(i+1)] = model.predict(test_x)\n",
    "    names['inv_yhat_%s' % str(i+1)] = scaler1.inverse_transform(names['y_hat_%s' % str(i+1)])\n",
    "\n",
    "    test_y = pd.DataFrame(test_y)\n",
    "    names['inv_y_%s' % str(i+1)] = scaler1.inverse_transform(test_y)\n",
    "\n",
    "    names['rmse_%s' % str(i+1)] = evaluate(names['inv_y_%s' % str(i+1)],names['inv_yhat_%s' % str(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = []\n",
    "for i in range(16):\n",
    "    rmse.append(names['rmse_%s'%str(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse=np.array(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92852077, 1.35780661, 1.66447952, 2.23584927, 2.3967724 ,\n",
       "       2.57332476, 2.61378391, 2.69889938, 2.92419248, 2.96987496,\n",
       "       2.85015941, 2.61763551, 2.68451787, 2.58265647, 2.58631418,\n",
       "       2.64123329])"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95357396, 0.93210967, 0.91677602, 0.88820754, 0.88016138,\n",
       "       0.87133376, 0.8693108 , 0.86505503, 0.85379038, 0.85150625,\n",
       "       0.85749203, 0.86911822, 0.86577411, 0.87086718, 0.87068429,\n",
       "       0.86793834])"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_ = 1 - rmse/20\n",
    "acc_.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cdann = []\n",
    "for i in range(16):\n",
    "    y_cdann.append(names['inv_yhat_%s' % str(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_cdann = pd.DataFrame(y_cdann)\n",
    "y_cdann = pd.DataFrame(np.array(y_cdann).reshape(16,-1).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 未分类的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30536 samples, validate on 3393 samples\n",
      "Epoch 1/100\n",
      "30536/30536 [==============================] - 1s 43us/step - loss: 0.1343 - val_loss: 0.1071\n",
      "Epoch 2/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0888 - val_loss: 0.0569\n",
      "Epoch 3/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0677 - val_loss: 0.0477\n",
      "Epoch 4/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0599 - val_loss: 0.0416\n",
      "Epoch 5/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0520 - val_loss: 0.0365\n",
      "Epoch 6/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0473 - val_loss: 0.0341\n",
      "Epoch 7/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0446 - val_loss: 0.0320\n",
      "Epoch 8/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0421 - val_loss: 0.0311\n",
      "Epoch 9/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0407 - val_loss: 0.0312\n",
      "Epoch 10/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0397 - val_loss: 0.0302\n",
      "Epoch 11/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0393 - val_loss: 0.0302\n",
      "Epoch 12/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0391 - val_loss: 0.0300\n",
      "Epoch 13/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0385 - val_loss: 0.0303\n",
      "Epoch 14/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0381 - val_loss: 0.0299\n",
      "Epoch 15/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0381 - val_loss: 0.0295\n",
      "Epoch 16/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0376 - val_loss: 0.0295\n",
      "Epoch 17/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0376 - val_loss: 0.0293\n",
      "Epoch 18/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0372 - val_loss: 0.0285\n",
      "Epoch 19/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0371 - val_loss: 0.0282\n",
      "Epoch 20/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0366 - val_loss: 0.0279\n",
      "Epoch 21/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0363 - val_loss: 0.0275\n",
      "Epoch 22/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0364 - val_loss: 0.0274\n",
      "Epoch 23/100\n",
      "30536/30536 [==============================] - 0s 9us/step - loss: 0.0359 - val_loss: 0.0269\n",
      "Epoch 24/100\n",
      "30536/30536 [==============================] - 0s 10us/step - loss: 0.0357 - val_loss: 0.0270\n",
      "Epoch 25/100\n",
      "30536/30536 [==============================] - 0s 11us/step - loss: 0.0357 - val_loss: 0.0267\n",
      "Epoch 26/100\n",
      "30536/30536 [==============================] - 0s 10us/step - loss: 0.0357 - val_loss: 0.0263\n",
      "Epoch 27/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0355 - val_loss: 0.0265\n",
      "Epoch 28/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0354 - val_loss: 0.0263\n",
      "Epoch 29/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0352 - val_loss: 0.0268\n",
      "Epoch 30/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0352 - val_loss: 0.0258\n",
      "Epoch 31/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0352 - val_loss: 0.0268\n",
      "Epoch 32/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0350 - val_loss: 0.0254\n",
      "Epoch 33/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0343 - val_loss: 0.0245\n",
      "Epoch 34/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0342 - val_loss: 0.0243\n",
      "Epoch 35/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0339 - val_loss: 0.0241\n",
      "Epoch 36/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0338 - val_loss: 0.0240\n",
      "Epoch 37/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0336 - val_loss: 0.0237\n",
      "Epoch 38/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0338 - val_loss: 0.0243\n",
      "Epoch 39/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0336 - val_loss: 0.0240\n",
      "Epoch 40/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0336 - val_loss: 0.0239\n",
      "Epoch 41/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0335 - val_loss: 0.0235\n",
      "Epoch 42/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0336 - val_loss: 0.0239\n",
      "Epoch 43/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0335 - val_loss: 0.0237\n",
      "Epoch 44/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0336 - val_loss: 0.0240\n",
      "Epoch 45/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0333 - val_loss: 0.0235\n",
      "Epoch 46/100\n",
      "30536/30536 [==============================] - 0s 7us/step - loss: 0.0335 - val_loss: 0.0237\n",
      "Epoch 47/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0334 - val_loss: 0.0241\n",
      "Epoch 48/100\n",
      "30536/30536 [==============================] - 0s 7us/step - loss: 0.0331 - val_loss: 0.0236\n",
      "Epoch 49/100\n",
      "30536/30536 [==============================] - 0s 7us/step - loss: 0.0333 - val_loss: 0.0242\n",
      "Epoch 50/100\n",
      "30536/30536 [==============================] - 0s 8us/step - loss: 0.0333 - val_loss: 0.0235\n",
      "detrended data \n",
      " [1.34271134 1.58456431 1.81100532 2.03878527 2.23893768 2.41869182\n",
      " 2.56708094 2.6910104  2.78847479 2.86667474 2.93337842 3.00326721\n",
      " 3.07568501 3.16612559 3.28011659 3.4196056 ] \n",
      " [0.93286443 0.92077178 0.90944973 0.89806074 0.88805312 0.87906541\n",
      " 0.87164595 0.86544948 0.86057626 0.85666626 0.85333108 0.84983664\n",
      " 0.84621575 0.84169372 0.83599417 0.82901972]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "import datetime\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# parameters\n",
    "input_len = 48\n",
    "output_step = 16\n",
    "num_feature = 1\n",
    "batch_size = 512\n",
    "epochs = 100\n",
    "P_cap_66= 20\n",
    "\n",
    "# load data\n",
    "data_ = pd.read_csv('power_detrended/66.csv', index_col = 3, parse_dates=True)\n",
    "power = pd.DataFrame(data_['power_with_trend'])\n",
    "# power = pd.DataFrame(data['power_with_trend'])\n",
    "\n",
    "# # choose 7.00-19.00 data\n",
    "# name = power.columns\n",
    "# data_chosen = list()\n",
    "# for i in range(len(power)):\n",
    "#     if i%96<=76 and i%96>28:\n",
    "#         data_chosen.append(power.iloc[i].values.reshape(1,-1))\n",
    "# power =  np.concatenate(data_chosen, axis=0)\n",
    "# power = power[(power.index.time<=datetime.time(19,0)) & (power.index.time>=datetime.time(7,0))]\n",
    "# power = pd.DataFrame(power)\n",
    "# power.columns = name\n",
    "\n",
    "# normalize features\n",
    "scaler1 = MinMaxScaler(feature_range=(0,1))\n",
    "power = pd.DataFrame(scaler1.fit_transform(power))\n",
    "\n",
    "cols, names = list(), list()\n",
    "\n",
    "# generate data\n",
    "for i in range(input_len, 0 ,-1):\n",
    "    cols.append(power.shift(i))\n",
    "    names += [('df(t-%d)' % i)]\n",
    "\n",
    "for i in range(output_step):\n",
    "    cols.append(power.shift(-i))\n",
    "    names += [('df(t+%d)' % i)]\n",
    "\n",
    "data = pd.concat(cols, axis=1)\n",
    "data.columns = names\n",
    "data.dropna(inplace=True)\n",
    "# data = data.values\n",
    "\n",
    "# generate inputs and outputs\n",
    "idx_of_input = list(range(16))+list(range(0,-8,-1))\n",
    "input = data.iloc[:, :input_len]\n",
    "input = input.iloc[:,idx_of_input]\n",
    "output = data.iloc[:, -16:]\n",
    "\n",
    "# split train and test(test one month)\n",
    "k = -1000\n",
    "train_x,train_y = input[:k],output[:k]\n",
    "test_x,test_y = input[k:],output[k:]\n",
    "\n",
    "input_dim = len(idx_of_input)\n",
    "# design network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(int(input_dim/2),input_dim=input_dim , activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history = model.fit(train_x,train_y,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    validation_split = 0.1,\n",
    "                    callbacks=[\n",
    "                        TensorBoard(log_dir='/tmp/tensorboard', write_graph=True),\n",
    "                        EarlyStopping(monitor='val_loss', patience=5, mode='auto')\n",
    "                    ]\n",
    "                    )\n",
    "\n",
    "# make a prediction\n",
    "y_hat = model.predict(test_x)\n",
    "inv_yhat = scaler1.inverse_transform(y_hat)\n",
    "\n",
    "# test_y = test_y.reshape((len(test_y), output_step))\n",
    "inv_y = scaler1.inverse_transform(test_y)\n",
    "\n",
    "#calculate rmse\n",
    "y_true = test_y.values\n",
    "rmse = np.sqrt(np.average((inv_y-inv_yhat)**2,axis = 0))\n",
    "acc = 1 - rmse/P_cap_66\n",
    "\n",
    "print('detrended data','\\n',rmse,'\\n',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 采用原始数据的SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "index = power_66.index\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "power_66_normalized = pd.DataFrame(scaler.fit_transform(power_66))\n",
    "power_66_normalized['index'] = index\n",
    "power_66_normalized = power_66_normalized.set_index('index')\n",
    "\n",
    "power_66_supervised = series_to_supervised(power_66_normalized, 48, 16)\n",
    "\n",
    "index_of_input = list(range(16)) + list(range(-8,0,1))\n",
    "# index_of_input =  list(range(-8,0,1))\n",
    "inputs = power_66_supervised.iloc[:,:48]\n",
    "inputs = inputs.iloc[:,index_of_input]\n",
    "outputs = power_66_supervised.iloc[:,48:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    names['output_%s'%str(i+1)] = outputs.iloc[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 1000\n",
    "test_len = 1000\n",
    "for i in range(16):\n",
    "    \n",
    "    # split train and test(test one month)\n",
    "    train_x,train_y = inputs[:train_len],names['output_%s'%str(i+1)][:train_len]\n",
    "    test_x,test_y = inputs[1000:1000+test_len],names['output_%s'%str(i+1)][1000:1000+test_len]\n",
    "\n",
    "    clf = SVR(kernel='rbf',\n",
    "              gamma = 'auto'\n",
    "              )\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    names['y_hat_%s' % str(i+1)] = clf.predict(test_x)\n",
    "\n",
    "    #calculate rmse\n",
    "    names['inv_yhat_%s' % str(i+1)] = scaler.inverse_transform(names['y_hat_%s' % str(i+1)].reshape(-1,1))\n",
    "\n",
    "    test_y = pd.DataFrame(test_y)\n",
    "    names['inv_y_%s' % str(i+1)] = scaler.inverse_transform(test_y)\n",
    "\n",
    "    names['rmse_%s' % str(i+1)] = evaluate(names['inv_y_%s' % str(i+1)],names['inv_yhat_%s' % str(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.36654403, 1.50871801, 1.61602001, 1.72386663, 1.86571657,\n",
       "       1.98901452, 2.13909218, 2.2877851 , 2.43125011, 2.55549577,\n",
       "       2.65376621, 2.7656417 , 2.84619928, 2.94083772, 3.01392379,\n",
       "       3.13264301])"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_svr = []\n",
    "for i in range(16):\n",
    "    rmse_svr.append(names['rmse_%s'%str(i+1)])\n",
    "rmse_svr = np.array(rmse_svr).reshape(-1)\n",
    "rmse_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9316728 , 0.9245641 , 0.919199  , 0.91380667, 0.90671417,\n",
       "       0.90054927, 0.89304539, 0.88561075, 0.87843749, 0.87222521,\n",
       "       0.86731169, 0.86171792, 0.85769004, 0.85295811, 0.84930381,\n",
       "       0.84336785])"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = 1-rmse_svr/20\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类后的SVR（波动小的类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# names=locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_input_svr = list(range(16)) + list(range(-8,0,1))\n",
    "for i in range(16):\n",
    "    \n",
    "    # split train and test(test one month)\n",
    "    names['input_stable_svr_%s'%str(i+1)] = names['input_stable_%s' % str(i+1)].iloc[:,index_of_input_svr]\n",
    "    train_x_svr,train_y_svr = names['input_stable_svr_%s' % str(i+1)][:10000],names['output_stable_%s' % str(i+1)][:10000]\n",
    "    test_x_svr,test_y_svr = names['input_stable_svr_%s' % str(i+1)][-1000:],names['output_stable_%s' % str(i+1)][-1000:]\n",
    "    \n",
    "    clf2 = SVR(kernel='rbf',\n",
    "              gamma = 'auto',\n",
    "#               verbose=True\n",
    "              )\n",
    "\n",
    "    clf2.fit(train_x_svr, train_y_svr)\n",
    "\n",
    "    names['y_hat_svr_%s' % str(i+1)] = clf2.predict(test_x_svr)\n",
    "\n",
    "    #calculate rmse\n",
    "    names['inv_yhat_svr_%s' % str(i+1)] = scaler.inverse_transform(names['y_hat_svr_%s' % str(i+1)].reshape(-1,1))\n",
    "\n",
    "    test_y_svr = pd.DataFrame(test_y_svr)\n",
    "    names['inv_y_svr_%s' % str(i+1)] = scaler.inverse_transform(test_y_svr)\n",
    "\n",
    "    names['rmse_svr_%s' % str(i+1)] = evaluate(names['inv_y_svr_%s' % str(i+1)],names['inv_yhat_svr_%s' % str(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2704821 , 1.5511445 , 1.77972293, 2.00550682, 2.24999066,\n",
       "       2.40297764, 2.51927151, 2.57632818, 2.79009476, 2.77527013,\n",
       "       2.69567767, 2.61057294, 2.51489202, 2.516098  , 2.62445597,\n",
       "       2.7205085 ])"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_svr = []\n",
    "for i in range(16):\n",
    "    rmse_svr.append(names['rmse_svr_%s'%str(i+1)])\n",
    "rmse_svr = np.array(rmse_svr).reshape(-1)\n",
    "rmse_svr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93647589, 0.92244278, 0.91101385, 0.89972466, 0.88750047,\n",
       "       0.87985112, 0.87403642, 0.87118359, 0.86049526, 0.86123649,\n",
       "       0.86521612, 0.86947135, 0.8742554 , 0.8741951 , 0.8687772 ,\n",
       "       0.86397457])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_svr = 1-rmse_svr/20\n",
    "acc_svr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型对训练样本和测试样本的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型搭建，参数选择："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# normalize features\n",
    "index = power_66.index\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "power_66_normalized = pd.DataFrame(scaler.fit_transform(power_66))\n",
    "power_66_normalized['index'] = index\n",
    "power_66_normalized = power_66_normalized.set_index('index')\n",
    "\n",
    "power_66_supervised = series_to_supervised(power_66_normalized, 48, 16)\n",
    "\n",
    "index_of_input = list(range(16)) + list(range(-8,0,1))\n",
    "# index_of_input =  list(range(-8,0,1))\n",
    "inputs = power_66_supervised.iloc[:,:48]\n",
    "inputs = inputs.iloc[:,index_of_input]\n",
    "outputs = power_66_supervised.iloc[:,48:]\n",
    "\n",
    "for i in range(16):\n",
    "    names['output_%s'%str(i+1)] = outputs.iloc[:,i]\n",
    "\n",
    "train_len = 1000\n",
    "test_len = 1000\n",
    "\n",
    "# 构建测试的集合\n",
    "train_x,train_y = inputs[:train_len],names['output_%s'%str(i+1)][:train_len]\n",
    "\n",
    "clf = SVR(kernel='rbf',\n",
    "          gamma = 'auto'\n",
    "          )\n",
    "\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建测试的集合\n",
    "test_x,test_y = inputs[train_len:],names['output_%s'%str(i+1)][train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_svr = []\n",
    "rmse_svr = []\n",
    "for i in range(16):\n",
    "\n",
    "    names['y_hat_%s' % str(i+1)] = clf.predict(inputs)\n",
    "\n",
    "    #calculate rmse\n",
    "    names['inv_yhat_%s' % str(i+1)] = scaler.inverse_transform(names['y_hat_%s' % str(i+1)].reshape(-1,1))\n",
    "\n",
    "    names['output_%s'%str(i+1)] = pd.DataFrame(names['output_%s'%str(i+1)])\n",
    "    names['inv_y_%s' % str(i+1)] = scaler.inverse_transform(names['output_%s'%str(i+1)])\n",
    "\n",
    "    names['rmse_%s' % str(i+1)] = evaluate(names['inv_y_%s' % str(i+1)],names['inv_yhat_%s' % str(i+1)])\n",
    "    rmse_svr.append(names['rmse_%s'%str(i+1)])\n",
    "    \n",
    "    names['error_%s' % str(i+1)] = np.abs(names['inv_y_%s' % str(i+1)]-names['inv_yhat_%s' % str(i+1)])\n",
    "    error_svr.append(names['error_%s' % str(i+1)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_svr = np.array(error_svr)\n",
    "# error_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2017-01-03 07:15:00</td>\n",
       "      <td>10.150079</td>\n",
       "      <td>10.150079</td>\n",
       "      <td>10.150079</td>\n",
       "      <td>10.150079</td>\n",
       "      <td>10.150079</td>\n",
       "      <td>9.959079</td>\n",
       "      <td>9.383079</td>\n",
       "      <td>8.786079</td>\n",
       "      <td>7.720079</td>\n",
       "      <td>7.464079</td>\n",
       "      <td>5.396079</td>\n",
       "      <td>2.689079</td>\n",
       "      <td>0.748079</td>\n",
       "      <td>0.722921</td>\n",
       "      <td>1.937921</td>\n",
       "      <td>2.704921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-03 07:30:00</td>\n",
       "      <td>10.792310</td>\n",
       "      <td>10.792310</td>\n",
       "      <td>10.792310</td>\n",
       "      <td>10.792310</td>\n",
       "      <td>10.601310</td>\n",
       "      <td>10.025310</td>\n",
       "      <td>9.428310</td>\n",
       "      <td>8.362310</td>\n",
       "      <td>8.106310</td>\n",
       "      <td>6.038310</td>\n",
       "      <td>3.331310</td>\n",
       "      <td>1.390310</td>\n",
       "      <td>0.080690</td>\n",
       "      <td>1.295690</td>\n",
       "      <td>2.062690</td>\n",
       "      <td>2.851690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-03 07:45:00</td>\n",
       "      <td>11.296694</td>\n",
       "      <td>11.296694</td>\n",
       "      <td>11.296694</td>\n",
       "      <td>11.105694</td>\n",
       "      <td>10.529694</td>\n",
       "      <td>9.932694</td>\n",
       "      <td>8.866694</td>\n",
       "      <td>8.610694</td>\n",
       "      <td>6.542694</td>\n",
       "      <td>3.835694</td>\n",
       "      <td>1.894694</td>\n",
       "      <td>0.423694</td>\n",
       "      <td>0.791306</td>\n",
       "      <td>1.558306</td>\n",
       "      <td>2.347306</td>\n",
       "      <td>3.221306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-03 08:00:00</td>\n",
       "      <td>11.745375</td>\n",
       "      <td>11.745375</td>\n",
       "      <td>11.554375</td>\n",
       "      <td>10.978375</td>\n",
       "      <td>10.381375</td>\n",
       "      <td>9.315375</td>\n",
       "      <td>9.059375</td>\n",
       "      <td>6.991375</td>\n",
       "      <td>4.284375</td>\n",
       "      <td>2.343375</td>\n",
       "      <td>0.872375</td>\n",
       "      <td>0.342625</td>\n",
       "      <td>1.109625</td>\n",
       "      <td>1.898625</td>\n",
       "      <td>2.772625</td>\n",
       "      <td>3.220625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-03 08:15:00</td>\n",
       "      <td>12.112310</td>\n",
       "      <td>11.921310</td>\n",
       "      <td>11.345310</td>\n",
       "      <td>10.748310</td>\n",
       "      <td>9.682310</td>\n",
       "      <td>9.426310</td>\n",
       "      <td>7.358310</td>\n",
       "      <td>4.651310</td>\n",
       "      <td>2.710310</td>\n",
       "      <td>1.239310</td>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.742690</td>\n",
       "      <td>1.531690</td>\n",
       "      <td>2.405690</td>\n",
       "      <td>2.853690</td>\n",
       "      <td>3.215690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0          1          2          3          4  \\\n",
       "index                                                                        \n",
       "2017-01-03 07:15:00  10.150079  10.150079  10.150079  10.150079  10.150079   \n",
       "2017-01-03 07:30:00  10.792310  10.792310  10.792310  10.792310  10.601310   \n",
       "2017-01-03 07:45:00  11.296694  11.296694  11.296694  11.105694  10.529694   \n",
       "2017-01-03 08:00:00  11.745375  11.745375  11.554375  10.978375  10.381375   \n",
       "2017-01-03 08:15:00  12.112310  11.921310  11.345310  10.748310   9.682310   \n",
       "\n",
       "                             5         6         7         8         9  \\\n",
       "index                                                                    \n",
       "2017-01-03 07:15:00   9.959079  9.383079  8.786079  7.720079  7.464079   \n",
       "2017-01-03 07:30:00  10.025310  9.428310  8.362310  8.106310  6.038310   \n",
       "2017-01-03 07:45:00   9.932694  8.866694  8.610694  6.542694  3.835694   \n",
       "2017-01-03 08:00:00   9.315375  9.059375  6.991375  4.284375  2.343375   \n",
       "2017-01-03 08:15:00   9.426310  7.358310  4.651310  2.710310  1.239310   \n",
       "\n",
       "                           10        11        12        13        14  \\\n",
       "index                                                                   \n",
       "2017-01-03 07:15:00  5.396079  2.689079  0.748079  0.722921  1.937921   \n",
       "2017-01-03 07:30:00  3.331310  1.390310  0.080690  1.295690  2.062690   \n",
       "2017-01-03 07:45:00  1.894694  0.423694  0.791306  1.558306  2.347306   \n",
       "2017-01-03 08:00:00  0.872375  0.342625  1.109625  1.898625  2.772625   \n",
       "2017-01-03 08:15:00  0.024310  0.742690  1.531690  2.405690  2.853690   \n",
       "\n",
       "                           15  \n",
       "index                          \n",
       "2017-01-03 07:15:00  2.704921  \n",
       "2017-01-03 07:30:00  2.851690  \n",
       "2017-01-03 07:45:00  3.221306  \n",
       "2017-01-03 08:00:00  3.220625  \n",
       "2017-01-03 08:15:00  3.215690  "
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(error_svr.reshape(16,-1).T, index = inputs.index).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.60608138, 7.36207737, 7.09835234, 6.81881753, 6.52802929,\n",
       "       6.2299554 , 5.92932101, 5.63049725, 5.34212915, 5.07125385,\n",
       "       4.82928668, 4.62460015, 4.46800029, 4.37057074, 4.33808484,\n",
       "       4.37387198])"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_svr = np.array(rmse_svr).reshape(-1)\n",
    "rmse_svr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ],\n",
       "       [ 0.    ],\n",
       "       [ 0.    ],\n",
       "       ...,\n",
       "       [14.6068],\n",
       "       [13.1864],\n",
       "       [12.084 ]])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00032161e+01],\n",
       "       [ 1.08584309e+01],\n",
       "       [ 1.14170861e+01],\n",
       "       ...,\n",
       "       [-1.20526475e-01],\n",
       "       [ 9.01644681e-03],\n",
       "       [ 6.73468610e-02]])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
